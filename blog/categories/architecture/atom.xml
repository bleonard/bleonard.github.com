<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: architecture | BLog]]></title>
  <link href="http://bleonard.github.io/blog/categories/architecture/atom.xml" rel="self"/>
  <link href="http://bleonard.github.io/"/>
  <updated>2017-04-14T15:13:13-07:00</updated>
  <id>http://bleonard.github.io/</id>
  <author>
    <name><![CDATA[Brian Leonard]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Architecture: Consider Kron]]></title>
    <link href="http://bleonard.github.io/blog/2017/04/14/architecture-consider-kron/"/>
    <updated>2017-04-14T00:00:00-07:00</updated>
    <id>http://bleonard.github.io/blog/2017/04/14/architecture-consider-kron</id>
    <content type="html"><![CDATA[<p>The last post in our architecture series discussed <a href="/blog/2017/03/17/architecture-background-processing/">background processing</a>. There is a special type of background processing that I wanted to make a quick note about. These are things that need to be done periodically or otherwise on a schedule.</p>

<p>In our internal speak, we call this a "kron" job. If you are familiar with <a href="https://en.wikipedia.org/wiki/Cron">cron</a> jobs, it's the same idea. A product manager misspelled it once and it stuck! We don't actually use regular cron infrastructure, so the spelling <em>nuance</em> is helpful.</p>

<p>The specifics of how we implement it involve our <a href="/blog/2015/04/02/queue-bus/">message bus</a> infrastructure, but I think the concept and the decisions involved could include many other implementations.</p>

<h3>When to use it</h3>

<p>Let's take the job from the <a href="/blog/2017/03/17/architecture-background-processing/">previous article</a>. The "charge an invoice 24 hours later" case is an interesting one. The system certainly supports delaying that code to run for an arbitrary time, but that's not always the best idea.</p>

<p>```ruby
class InvoiceChargeWorker
  include TResque::Worker
  inputs :invoice_id</p>

<p>  worker_lock :invoice_id</p>

<p>  def work</p>

<pre><code>return unless needed?
invoice.charge!
</code></pre>

<p>  end</p>

<p>  def to_id</p>

<pre><code>invoice.to_id
</code></pre>

<p>  end</p>

<p>  def needed?</p>

<pre><code>invoice.pending?
</code></pre>

<p>  end</p>

<p>  def invoice</p>

<pre><code>@invoice ||= Invoice.find(invoice_id)
</code></pre>

<p>  end
end</p>

<h1>When invoice is created</h1>

<p>InvoiceChargeWorker.enqueue_at(24.hours.from_now, invoice_id: invoice.id)
```</p>

<p>One reason would be memory. When there a lot of invoices (woot!), we still have to save the notion of what should be done somewhere until it gets processed. In this case, the <a href="https://redis.io/">Redis</a> instance will have it stored in memory. The memory could fill up and adding more workers won't help because of the delay.</p>

<p>The second reason is stability. This is important stuff and Redis could have issues and lose the data. We made everything <a href="http://www.restapitutorial.com/lessons/idempotency.html">idempotent</a> and could recreate everything, but it would certainly be a huge hassle.</p>

<p>So when enqueueing something to run in the future, especially if it is important or a long time from now (more than a few minutes), we consider kron.</p>

<h3>Batch mode</h3>

<p>If we were going to accomplish the same things but on a schedule, the code would have to change in some way. I like the existing worker because it already has the good stuff from the last article: source of truth, knowing whether or not it still needs to be run, and mutual exclusion. When batch processing, I believe it's also good to still operate on this one at a time where the count (memory for redis) is low or the risk of issues is high. Both are the case here.</p>

<p>To turn it into a batch processor we need to know what needs to be processed at any given moment. This is easy to determine because we have the <code>needed?</code> method. It looks to be invoices that are in the <code>pending</code> state. Sometimes we need to add a <code>state</code> column or other piece of data to know what needs to be in the batch but in this case we are good to go.</p>

<p>From there we can decide if we are going to update the class as-is or make a batch worker. A batch worker is its own worker and would look like this:</p>

<p>```ruby
class InvoiceChargeBatchWorker
  include TResque::Worker</p>

<p>  worker_lock :all
  queue_lock  :all</p>

<p>  def work</p>

<pre><code>Invoice.where(stat: 'pending').find_each do |invoice|
  InvoiceChargeWorker.enqueue(invoice_id: invoice.id)
end
</code></pre>

<p>  end
end</p>

<h1>process all pending invoices</h1>

<p>InvoiceChargeBatchWorker.enqueue()
```</p>

<p>That's it. Because of the worker lock on <code>InvoiceChargeWorker</code> and the state checking, it would be ok even if we were to enqueue it twice or something. Making a custom batch worker also prevents us from running this code twice.</p>

<p>We could also stick it as a class method on the original:</p>

<p>```ruby
class InvoiceChargeWorker
  include TResque::Worker
  inputs :invoice_id</p>

<p>  worker_lock :invoice_id</p>

<p>  def self.process_all!</p>

<pre><code>Invoice.where(stat: 'pending').find_each do |invoice|
  self.enqueue(invoice_id: invoice.id)
end
</code></pre>

<p>  end</p>

<p>  def work</p>

<pre><code>return unless needed?
invoice.charge!
</code></pre>

<p>  end</p>

<p>  def needed?</p>

<pre><code>invoice.pending?
</code></pre>

<p>  end</p>

<p>  def invoice</p>

<pre><code>@invoice ||= Invoice.find(invoice_id)
</code></pre>

<p>  end
end</p>

<h1>process all pending invoices</h1>

<p>InvoiceChargeWorker.process_all!
```</p>

<h3>How it works</h3>

<p>Again, in any given architecture there is probably a best way to do it. For example, maybe <a href="https://medium.com/airbnb-engineering/chronos-a-replacement-for-cron-f05d7d986a9d">this</a> is a good way to do it on top of <a href="http://mesos.apache.org/">Mesos</a>.</p>

<p>The challenge is running something on a schedule. In this case, process all invoices that need to be paid. That is what regular cron is made to do. However, we do not want to run that on every box. If we did that, we would have serious race conditions and might pay an invoice twice. Rather, we want to run it once globally across the entire infrastructure or at least per service.</p>

<p>We could probably do this by noting in the devops setup that one of the servers is special. It should get the cron setup. We could use something like the <a href="https://github.com/javan/whenever">whenever gem</a> to say what to do and we would only run that on one box per system. It needs to be per system because it has to be able to know what worker to enqueue or, in general, what code to run.</p>

<p>What we do instead is have a single service that has a process that sends out a heartbeat on the <a href="/blog/2015/04/02/queue-bus/">message bus</a>. Every minute, it publishes an event that looks like this.</p>

<p>```ruby
  # for Tue, 11 Apr 2017 00:25:00 UTC +00:00
  # epoch time: 1491870300</p>

<p>  QueueBus.publish(heartbeat_seconds", {</p>

<pre><code>"epoch_seconds"=&gt;1491870300,
"epoch_minutes"=&gt;24864505,
"epoch_hours"=&gt;414408,
"epoch_days"=&gt;17267,
"minute"=&gt;25,
"hour"=&gt;0, 
"day"=&gt;11,
"month"=&gt;4,
"year"=&gt;2017,
"yday"=&gt;101,
"wday"=&gt;2
</code></pre>

<p>  })
```</p>

<p>The current code for the process is already checked into <a href="https://github.com/queue-bus/queue-bus">queue-bus</a> and ready to use <a href="https://github.com/queue-bus/queue-bus/blob/5c009a3b2d2d58994813bdca0dc78547a18b0295/lib/queue_bus/heartbeat.rb">here</a>.</p>

<p><a href="https://github.com/queue-bus/resque-bus">Resque bus</a> supports this using the <a href="https://github.com/resque/resque-scheduler">resque-scheduler gem</a>. It is setup off by calling <code>QueueBus.heartbeat!</code>. We make sure it's setup every time we start up Resque.</p>

<p>```ruby
namespace :resque do
  task :setup => [:environment] do</p>

<pre><code>require 'resque_scheduler'
require 'resque/scheduler'
require 'tresque'

QueueBus.heartbeat!
</code></pre>

<p>  end
end
```</p>

<p>This <code>setup</code> is automatically called every time Resque starts.</p>

<h3>Usage</h3>

<p>So now we can subscribe to this event to run something every minute, hour, day, Monday, month, whatever.</p>

<p>```ruby</p>

<h1>every minute</h1>

<p>subscribe "every_minute", 'bus_event_type' => 'heartbeat_minutes' do |attributes|
  InvoiceChargeWorker.process_all!
end</p>

<h1>every hour: 4:22, 5:22, 6:22, etc</h1>

<p>subscribe "once_an_hour", 'bus_event_type' => 'heartbeat_minutes', 'minute' => 22 do |attributes|
  InvoiceChargeWorker.process_all!
end</p>

<h1>every day at 12:05 am</h1>

<p>subscribe "once_a_day", 'bus_event_type' => 'heartbeat_minutes', 'hour' => 0, 'minute' => 5 do |attributes|
  InvoiceChargeWorker.process_all!
end</p>

<h1>every monday at 1:52 am</h1>

<p>subscribe "early_monday_morning", 'bus_event_type' => 'heartbeat_minutes', 'wday' => 1, 'hour' => 1, 'minute' => 52 do |attributes|
  InvoiceChargeWorker.process_all!
end</p>

<h1>the 3rd of every month at 2:10 am</h1>

<p>subscribe "once_a_month", 'bus_event_type' => 'heartbeat_minutes', 'day' => 3, 'hour' => 2, 'minute' => 10 do |attributes|
  InvoiceChargeWorker.process_all!
end</p>

<h1>every 5 minutes: 4:00, 4:05, 4:10, etc</h1>

<p>subscribe "every 5 minutes" do |attributes|
  # if it doesn't fit the subscribe pattern, just subscribe to every minute and use ruby
  next unless attributes['minute'] % 5 == 0
  InvoiceChargeWorker.process_all!
end</p>

<p>```</p>

<h3>Summary</h3>

<p>So that is how "kron" works.</p>

<p>Over time, we have decided this is a much more reliable way to process items in the background when a delay is acceptable. By setting up some sort of centralized architecture for this, many services and subscribe in a way that is familiar and unsurprising. We have found a lot of value in that.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Architecture: Background Processing]]></title>
    <link href="http://bleonard.github.io/blog/2017/03/17/architecture-background-processing/"/>
    <updated>2017-03-17T00:00:00-07:00</updated>
    <id>http://bleonard.github.io/blog/2017/03/17/architecture-background-processing</id>
    <content type="html"><![CDATA[<p>So we have a bunch of <a href="/blog/2017/02/24/architecture-models/">models</a> and are doing stuff with them in <a href="/blog/2017/03/03/architecture-service-objects/">service objects</a>. The next thing we might need is to process some code in the background.</p>

<p>Not everything can be done inline from the API request. For example, we might need to geocode a user's postal code when they change it in their account. Or when an invoice is created, we want to charge it 24 hours later.</p>

<p>When working with background jobs, we default to the following practices:</p>

<ul>
<li>Workers are enqueued with a dictionary of inputs</li>
<li>These inputs should be used to fetch data from the source of truth</li>
<li>Workers know how to check if they still need to run</li>
<li>Locking schemes should protect parallel execution</li>
</ul>


<h3>Enqueue</h3>

<p>When we enqueue a worker, we have found that it's quite helpful to always use a dictionary (hash) of key/value pairs. <a href="https://github.com/resque/resque">Resque</a> and <a href="http://sidekiq.org/">Sidekiq</a> both take a list of arguments like <a href="https://github.com/mperham/sidekiq/wiki/Getting-Started">so</a>:</p>

<p>```ruby
class HardWorker
  include Sidekiq::Worker
  def perform(name, count)</p>

<pre><code># do something with name, count
</code></pre>

<p>  end
end</p>

<h1>enqueue</h1>

<p>HardWorker.perform_async('bob', 5)
```</p>

<p>This has proved to be problematic when adding new parameters or having optional parameters. For example, if we add a new (third) input parameter, there might be stuff in the queue with the old two. When the new code gets deployed, it will throw an 'invalid number of arguments' type of error. When using a hash, we can give it a default, fail gracefully, or do whatever we like on a class by class basis.</p>

<p>So to provide better change management and optional arguments, we always do it like so:</p>

<p>```ruby
class HardWorker
  include TResque::Worker
  inputs :name, :count</p>

<p>  def work</p>

<pre><code># do something with self.name, self.count
</code></pre>

<p>  end
end</p>

<h1>enqueue</h1>

<p>HardWorker.enqueue(name: 'bob', count: 5)
```</p>

<h3>Source of Truth</h3>

<p>Let's say we want to update a search index every time a user record is changed. We need to write their first name, last name, etc to <a href="https://www.elastic.co/">Elasticsearch</a>.</p>

<p>We could do something like this:</p>

<p>```ruby
class UserIndexWorker
  include TResque::Worker
  inputs :id, :first_name, :last_name, :etc</p>

<p>  def work</p>

<pre><code>Elasticsearch.index('users').write(id, id: id, first_name: first_name, last_name: last_name, etc: etc)
</code></pre>

<p>  end
end</p>

<h1>When user changes</h1>

<p>UserIndexWorker.enqueue(user.attributes.slice(:id, :first_name, :last_name, :etc))
```</p>

<p>This certainly would work, but is not considered best practice. It is better to be <a href="http://www.restapitutorial.com/lessons/idempotency.html">idempotent</a>. It writes everything that should ) by passing the minimal information to the background worker, who then looks up the source of truth. That way, if there is any delay between when it is enqueued and run, it will still send the correct information.</p>

<p>The better approach would look like this:</p>

<p>```ruby
class UserIndexWorker
  include TResque::Worker
  inputs :user_id</p>

<p>  def work</p>

<pre><code>Elasticsearch.index('users').write(user.attributes.slice(:id, :first_name, :last_name, :etc))
</code></pre>

<p>  end</p>

<p>  def user</p>

<pre><code>@user ||= User.find(user_id)
</code></pre>

<p>  end
end</p>

<h1>When user changes</h1>

<p>UserIndexWorker.enqueue(user_id: user.id)
```</p>

<p>In the same vein, the worker should be in charge of whether or not it needs to do anything in the first place. For example, we can enqueue a worker to run later about an <code>Invoice</code>. If, at that time, the payment is <code>Invoice</code> still should be charged, then charge it.</p>

<p>```ruby
class InvoiceChargeWorker
  include TResque::Worker
  inputs :invoice_id</p>

<p>  def work</p>

<pre><code>return unless needed?
invoice.charge!
</code></pre>

<p>  end</p>

<p>  def needed?</p>

<pre><code>invoice.pending?
</code></pre>

<p>  end</p>

<p>  def invoice</p>

<pre><code>@invoice ||= Invoice.find(invoice_id)
</code></pre>

<p>  end
end</p>

<h1>When invoice is created</h1>

<p>InvoiceChargeWorker.enqueue_at(24.hours.from_now, invoice_id: invoice.id)
```</p>

<p>This is another example of single source of truth. Even for jobs that are run immediately, this check is something we always put in place: return immediately if the worker is no longer relevant.</p>

<h3>Mutual Exclusion</h3>

<p>Let's say the <code>User</code> object can sometimes change a few times rapidly. The "source of truth" approach will make sure the right thing always gets indexed. So that's great. But it is pretty silly to index the same data twice or more times, right?</p>

<p>In this case, we add a queue lock. The effect is that if something is in the queue and waiting to be processed and you try to enqueue another one with the same inputs, then it will be a no-op. It looks like this:</p>

<p>```ruby
class UserIndexWorker
  include TResque::Worker
  inputs :user_id</p>

<p>  queue_lock :user_id
end
```</p>

<p>Another case that often arises is mutual exclusion for <em>runtime</em>. Maybe weird payment things happen to the payment service if two invoices for the same user are happening at the same time.</p>

<p>In this case, we add a worker lock. The effect is that if something is in the queue and about to start running and there is another running at that moment, then it will re-enqueue itself to run later. It looks like this:</p>

<p>```ruby
class InvoiceChargeWorker
  include TResque::Worker
  inputs :invoice_id</p>

<p>  worker_lock :to_id</p>

<p>  def work</p>

<pre><code>return unless needed?
invoice.charge!
</code></pre>

<p>  end</p>

<p>  def to_id</p>

<pre><code>invoice.to_id
</code></pre>

<p>  end</p>

<p>  def needed?</p>

<pre><code>invoice.pending?
</code></pre>

<p>  end</p>

<p>  def invoice</p>

<pre><code>@invoice ||= Invoice.find(invoice_id)
</code></pre>

<p>  end
end
```</p>

<p>For either type, you don't have to lock on all the attributes or can (as shown in the last example) use calculations. The namespace of the lock is the worker class name. You can also set the namespace to allow locking between different workers.</p>

<h3>Message Bus</h3>

<p>Our <a href="/blog/2015/04/02/queue-bus/">message bus</a> and our use of background processes have a lot in common. In fact, the message bus is built on top of the same background processing infrastructure. The question that arises is this: when should something be enqueued directly and when should it publish and respond to a bus subscription?</p>

<p>The first note is that you should <em>always be publishing</em> (ABP). It doesn't hurt anything to give (optional) visibility to other systems what is happening. Or use this as logging framework.</p>

<p>Just publishing, however, doesn't mean we have to use that to do work in the background. Be can bother publish and enqueue a background worker. We enqueue a worker when the work in the background is essential to the correct operation of the use case at hand.</p>

<p>One example to enqueue directly would be the geocoding worker I mentioned earlier: when the user gives a new postal code, figure out where that is. It's key to the account management system.</p>

<p>The search example I've been using might not actually be the best one because we would have the search system subscribed to changes in the account system. What I didn't show that the <code>enqueue</code> call might actually happen from within a subscription.</p>

<p><code>ruby
subscribe "user_changed" do |attributes|
  UserIndexWorker.enqueue(user_id: attributes['id'])
end
</code></p>

<p>So these two concepts can work together. Why not just index it right in the subscription, though? A primary reason might be to use some of the locking mechanisms as the bus does not have that. It also might be the case that the worker is enqueued from other locations and this keeps things DRY. The worker is also easier to unit test.</p>

<h3>TResque</h3>

<p>We use <a href="https://github.com/resque/resque">Resque</a> as a base foundation and built on top of it with an abstraction layer called <a href="https://github.com/taskrabbit/tresque">TResque</a>. That's TR (TaskRabbit) Resque. Get it? It puts all of these practices into place as well as adding and abstraction layer for the inevitable, but as yet unprioritized, move to <a href="http://sidekiq.org/">Sidekiq</a>.</p>

<p>I don't necessarily expect anyone to use this, but it doesn't hurt to make it available as an example of how we are using these tools.</p>

<p>You define a worker and enqueue things as show in the examples above. Then only layer left is around prioritization. You can give a queue name to a worker and then register what priority those workers are. If no queue is given, it is assumed to be the <code>default</code> queue.</p>

<p>```ruby
require 'tresque'</p>

<p>module Account
  class RegularWorker</p>

<pre><code>include ::TResque::Worker
# defaults to account_default queue
</code></pre>

<p>  end
end</p>

<p>module Account
  class RegularWorker</p>

<pre><code>include ::TResque::Worker
queue :refresh # lower priority account_refresh queue
</code></pre>

<p>  end
end</p>

<p>TResque.register("account") do
  queue :default, 100
  queue :refresh, -5000
end
```</p>

<p>Then when you run Resque, you can use these registrations to process the queues in the right order.</p>

<p>```ruby
require 'resque/tasks'
require 'resque_scheduler/tasks'
require "resque_bus/tasks"</p>

<p>namespace :resque do
  task :setup => [:environment] do</p>

<pre><code>require 'resque_scheduler'
require 'resque/scheduler'
require 'tresque'
</code></pre>

<p>  end</p>

<p>  task :queues => [:setup] do</p>

<pre><code>queues = ::TResque::Registry.queues
ENV["QUEUES"] = queues.join(",")
puts "TResque: #{ENV["QUEUES"]}"
</code></pre>

<p>  end
end</p>

<p>```</p>

<p><code>
  $ bundle exec rake resque:queues resque:work
  TResque: account_default, account_refresh
</code></p>

<p>This registration layer allows each of the systems (<a href="/blog/2014/02/11/rails-4-engines/">engines</a>) to work independently and still have centralized background processing.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Architecture: Surface Area]]></title>
    <link href="http://bleonard.github.io/blog/2017/03/10/architecture-surface-area/"/>
    <updated>2017-03-10T00:00:00-08:00</updated>
    <id>http://bleonard.github.io/blog/2017/03/10/architecture-surface-area</id>
    <content type="html"><![CDATA[<p>The last post in the <a href="https://www.taskrabbit.com">TaskRabbit</a> architecture series was about <a href="/blog/2017/03/03/architecture-service-objects/">service objects</a>. This an example of what I call minimizing "surface area" of the code.</p>

<p>Frankly, I might be using the term wrong. It seems possible "surface area" usually refers to API signature of some objects. What I'm talking about here is the following train of thought:</p>

<ul>
<li>I change or add a line of code</li>
<li>What did I just affect?</li>
</ul>


<p>The "surface area" is the other things I have to look over. It is the area that I have to make sure has appropriate test coverage. Having a large surface area is what slows down development teams. The goal is to minimize it.</p>

<h3>Service Objects</h3>

<p>So how does our use of service objects relate to this concept?</p>

<p>Let's say we have a new requirement that's applicable when a Tasker submits an invoice that modifies what gets saved. If I were to add the code to the <code>InvoiceJobOp</code> from the <a href="/blog/2017/03/03/architecture-service-objects/">previous article</a>, then it will only apply when the <code>Op</code> is run. If we were to do something in a <code>before_save</code> in the <code>Invoice</code> model, then it might accidentally kick in anytime an <code>Invoice</code> is changed.</p>

<p>That's a lot more tests and things to keep in our mind. If it is just in the <code>Op</code>, that is less of those kinds of debt, so adding in the <code>Op</code> is an example of minimizing the surface area of the change.</p>

<h3>Namespacing</h3>

<p>We went through a <a href="/blog/2015/10/06/v2-retrospective/">roundabout journey</a> to end up where were we are. Many of the changes were about surface area and trying to reduce it.</p>

<p>People like microservices and SOA because of this same principle. We tried it and that part of it worked out really well. There was just no way that a change in service A could affect service B. As <a href="/blog/2015/10/06/v2-retrospective/">discussed</a>, however, we ran into issues in other dimensions.</p>

<p>Our current use of <a href="/blog/2014/02/11/rails-4-engines/">engines</a> follows the same approach to achieve the same surface area effect. It is all about namespacing. Modifying the user management engine can not affect the marketplace engine. This allows us to proceed with more confidence when making such changes.</p>

<p>A particular aspect of our setup is that any given model is "owned" by only one engine. The rest of the engines are allowed to read from the database but they cannot write. This provides sanity and minimizes the surface area. For example, the validations only need to live in one spot. You also know that no other code can go rogue and start messing with the data by accident or otherwise.</p>

<h3>Bus</h3>

<p>Of course, the world isn't always cut and dry. Venn diagrams overlap. No abstraction or encapsulation is perfect. The seams in namespacing show up when something that happens in one service (engine) needs to affect something in another one.</p>

<p>For example, we were so happy just a few paragraphs ago that changes to the user management engine do not affect the marketplace engine. That is true and it is great. There is no direct effect from the code. However, as they tend to do, these pesky functional requirements always mess up perfect plans for the code. In this case, when a user changes their first name (in the account engine), the marketplace engine might need to update some data in <a href="https://www.elastic.co/">Elasticsearch</a>.</p>

<p>We use a <a href="/blog/2015/04/02/queue-bus/">message bus</a> to observe changes like this and react as appropriate.</p>

<p>```ruby</p>

<h1>Whenever the user changes</h1>

<p>subscribe 'user_may_have_changed', bus_observer_touched: 'user' do |attributes|
  # update the profile in ElasticSearch
  ProfileStoreWorker.enqueue(user_id: attributes['id'])
end
```</p>

<p>An important note here is that <code>ProfileStoreWorker</code> is <a href="http://www.restapitutorial.com/lessons/idempotency.html">idempotent</a>. It writes everything that should go in Elasticsearch every time. This technique reduces surface area by not depending on this single event and its contents, but rather only as a trigger.</p>

<p>One might say that these subscriptions are just as coupled as doing everything all in one spot. I see that point because, of course, the same things end up happening. However, we have this technique to be better for a few reasons.</p>

<ul>
<li>The trigger code (in the account engine) does not need to know about the rest of the system. It can mind its own business.</li>
<li>The subscribing code (in the marketplace engine) can be self-contained instead of being mixed up in the trigger code path.</li>
<li>Many different code paths might necessitate the <code>ProfileStoreWorker</code> to run. By decoupling it, we actually save complexity in many code paths.</li>
</ul>


<h3>Summary</h3>

<p>In code, developers tend to weave a tangled web wherein seemingly innocuous changes have far-reaching effects. We have been able to create more stable and agile code by considering the "surface area" of a change and minimizing it through some encapsulation and decoupling techniques.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Architecture: Service Objects]]></title>
    <link href="http://bleonard.github.io/blog/2017/03/03/architecture-service-objects/"/>
    <updated>2017-03-03T00:00:00-08:00</updated>
    <id>http://bleonard.github.io/blog/2017/03/03/architecture-service-objects</id>
    <content type="html"><![CDATA[<p>This is the second post in what is now indisputably a "series" of articles about how we build things at <a href="https://www.taskrabbit.com">TaskRabbit</a>. Over time, we/I have internalized all kinds of lessons and patterns and are trying to take the time to write some of the key things down.</p>

<p>Building upwards from the <a href="/blog/2017/02/24/architecture-models/">last article about models</a>, let's talk about how we use them. The models represent rows in the database in the <a href="http://guides.rubyonrails.org/active_record_basics.html">Rails ORM</a>. What code is deciding what to put in those rows and which ones should be created, etc? In our architecture, this role is filled by <a href="https://www.netguru.co/blog/service-objects-in-rails-will-help">service objects</a>.</p>

<p>Overall, we default to the following rules when using models in our system:</p>

<ul>
<li>Models contain data/state validations and methods tied directly to them</li>
<li>Models are manipulated by service objects that reflect the user experience</li>
</ul>


<h3>Something has to be fat</h3>

<p>In the beginning, there was <a href="http://ideum.com/2006/07/05/102/">Rails</a> and we saw that it was good. The world was optimized around the CRUD/REST use cases. Controllers had <code>update_attributes</code> and such. When there was more logic/nuance, it was put there in the controller (or the view).</p>

<p>There was a backlash of sorts against that and the new paradigm was <a href="http://weblog.jamisbuck.org/2006/10/18/skinny-controller-fat-model">"Fat model, skinny controller"</a>. The controllers were simple and emphasized workflow instead of business logic. Views were simpler. That stuff was put in the models. Model code was easier to reuse.</p>

<p>Thus arose the great <a href="http://blog.codeclimate.com/blog/2012/10/17/7-ways-to-decompose-fat-activerecord-models/">"God Model"</a> issue. Fat is one thing, but we had some seriously obese models. Things like <code>User</code> and <code>Task</code> simply had too much going on. We could put stuff in mixins/concerns but that didn't change the fact that there was tons of code that all could be subtly interacting with each other.</p>

<p>Business logic has to go somewhere. For us, that somewhere is in service objects.</p>

<h3>Operations</h3>

<p>In our architecture, we call them "Operations" and they extend a class called <code>Backend::Op</code>. This more or less uses the <a href="https://github.com/mnelson/subroutine">subroutine</a> gem.</p>

<p>Much can be read about what it means to be a service object, but here is my very scientific (Rails-specific) definition.</p>

<ul>
<li>Includes <code>ActiveModel</code> stuff like <code>Naming</code>, <code>Validations</code>, and <code>Callbacks</code></li>
<li>Allows declaration of what fields (input parameters) it uses</li>
<li>Reflects an action in the system like "sign up a user" or "invoice a job"</li>
<li>Does whatever it needs to do to accomplish the action when asked including updating or creating one or more models</li>
</ul>


<p>Here's a simplified example:</p>

<p>```ruby
class InvoiceJobOp &lt; ::Backend::Op
  include Mixins::AtomicOperation # all in same transaction</p>

<p>  field :hours
  field :job_id</p>

<p>  validates :job_id, presence: true
  validate  :validate_hour       # hours given
  validate  :validate_assignment # tasker is assigned
  # ... other checks</p>

<p>  def perform</p>

<pre><code>create_invoice!    # record hours and such
generate_payment!  # pending payment transaction
appointment_done!  # note that appointment completed

if ongoing?
  schedule_next_appointment! # schedule next if more
else
  complete_assignment!       # otherwise, no more
end

enqueue_background_workers!  # follow up later on stuff
</code></pre>

<p>  end
end</p>

<p>```</p>

<h3>No Side Effects</h3>

<p>When we followed the "Fat Model" pattern, we got what we wanted. This was usually methods in one of the models. Sometimes there were callbacks added. These were the most dangerous because they were happening on every <code>save</code>. Often, this added unnecessary side effects.</p>

<p>With the service object approach, it is very clear what is happening for the action at hand. When you "invoice a job," you create the invoice, generate the payment, mark the appointment done, schedule the next appointment, and enqueue some background workers.</p>

<p>This certainty leads to less technical and product debt. When something new needs to be added to this action, it's very clear where it goes.</p>

<h3>Errors</h3>

<p>Our <code>Op</code> class above does several model manipulations to the related invoices, appointments, etc. Each some of these does a <code>save</code> to something. Those <code>save</code> calls could raise errors. If any of those raise an error, then the <code>Op</code> itself will inherit it and it will be available on the <code>op.errors</code> method just like a normal <code>ActiveRecord</code> object.</p>

<p>This also allows chaining of operations. If there was a <code>ScheduleAppointmentOp</code> class, it could be used in the above <code>schedule_next_appointment!</code> method. If it raised an error, it would propagate to the <code>InvoiceJobOp</code>.</p>

<h3>Controllers</h3>

<p>Generally speaking, we have one <code>Op</code> per controller action that declares what it expects and manipulates the backend data as needed.</p>

<p>Here is a typical example from one of our controllers.</p>

<p>```ruby
class JobsController &lt; ApplicationController
  def confirm</p>

<pre><code>@job = Job.find(params[:id])
authorize @job, :confirm? # authorization
op = Organic::JobConfirmOp.new(current_user)
op.submit!(params.merge(job_id: @job.id)) # perform action
render :show # render template
</code></pre>

<p>  end
end
```</p>

<p>An action will typically do the following:</p>

<ul>
<li>Load a resource</li>
<li>Authorize the user is allowed do do an action</li>
<li>Perform the action with an operation (other things are in place to render and error if the op fails)</li>
<li>Render a template</li>
</ul>


<p>Note that this is clearly not a typical RESTful route. We've found that becomes less important when using this pattern. When the controllers are just wiring things up and are all a 5 lines or less, it feels like there is more flexibility.</p>

<p>It probably gets summed up something like this: wherever the fat (real work) is, that should be focused. For us, it's not the controller because of service objects. The real work is 1 to 1 focused with the use case. If more was in the controllers, we'd probably be closer to the standard index, show, etc methods because of the focus concept.</p>

<h3>Sharing</h3>

<p>So we have pushed everything out closer to the user experience and away from the models. But what if something is needed in a few pieces of the experience?</p>

<p>A few ways we have done sharing:</p>

<ul>
<li>Two <code>Op</code>s can use a lower-level one or other type of class as noted above.</li>
<li>Two <code>Op</code>s can have a mixin with the shared behavior.</li>
<li>We can add a method to an applicable model. We tend to do this on simple methods that are interpreting the model data to answer a commonly-asked question or commonly-used display value.</li>
</ul>


<h3>Summary</h3>

<p>We have found that this approach provides a more maintainable and overall successful way of building Rails apps.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Architecture: Models]]></title>
    <link href="http://bleonard.github.io/blog/2017/02/24/architecture-models/"/>
    <updated>2017-02-24T00:00:00-08:00</updated>
    <id>http://bleonard.github.io/blog/2017/02/24/architecture-models</id>
    <content type="html"><![CDATA[<p>This is the first post in what I hope will be a series of articles about how we build things at <a href="https://www.taskrabbit.com">TaskRabbit</a>. Over time, we/I have internalized all kinds of lessons and patterns, but have never written them down explicitly and publicly. So let's give that a try.</p>

<p>I thought we'd start with models. That's what <a href="http://api.rubyonrails.org/classes/ActiveRecord/Base.html">Rails</a> calls database tables where each row is an instance of that model class.</p>

<p>Overall, we default to the following rules when designing the models in a system:</p>

<ul>
<li>Keep the scope small and based on decisions in the workflow</li>
<li>Use state machines to declare and lock in the valid transitions</li>
<li>Denormalize as needed to optimize use cases in the experience</li>
</ul>


<h3>Scope</h3>

<p>When designing a feature (or the whole app in its early days), you have to decide what the models represent. I'm calling that the "scope" of the model.</p>

<p>For example, most applications have a <code>User</code> model. What columns will it have? Stuff about the user, obviously. But what stuff? One of the tradeoffs to consider is <code>User</code> vs. <code>Account</code> vs. <code>Profile</code>. If you put everything about the user in the same table as the one that's pointed to in many foreign keys through the system, there will be a performance impact.</p>

<p>So we put the most commonly needed items on every screen load in the <code>User</code> model and "extra" stuff in the <code>Profile</code>.</p>

<ul>
<li><code>User</code>: authentication, name, avatar, state</li>
<li><code>Profile</code>: address, average rating, bio information</li>
</ul>


<p>There are plenty of ways to cut this up into other models and move things around, but that's what I mean about "scope" of a model.</p>

<h3>States</h3>

<p>State machines are built into the foundation of the system. Almost every model has a <code>state</code> column and an initial state. There are then valid transitions to other states.</p>

<p>For example, there is a <code>PaymentTransaction</code> model. It has an initial "pending" state that represents the time between when an invoice is submitted and when we charge the credit card. During this time, it can move to a "canceled" state if it should not happen. Or, if things go as planned, it can transition to a "settled" state. After that, if there is an issue of some sort, it would go to a "refunded" state. Notably, going from "pending" to "refunded" is <em>not</em> a valid transition.</p>

<div class="jumbotron">
<img src="http://bleonard.github.io/images/posts/architecture-models/states.png" class="bigPicture" />
</div>


<p>Creating these state and transitions preserves some sanity in the system. It's a safety check. By asserting what is possible, we can (try to) prevent things that should not be possible.</p>

<h3>Nouns and Verbs</h3>

<p>The TaskRabbit marketplace creates a job that is sent to a Tasker. The Tasker can chat with the Client and can say they will do the job. Or they can decline. If they agree, they are officially assigned to the job and make an appointment. When they complete the job, they invoice the Client for the time worked. In most cases, it's done at that point. In other cases, it is "ongoing" where they come back next week (to clean again, for example). At more or less any time, the whole thing can be canceled.</p>

<p>If given that description, you could come up with many possible model structures. They would all have a set of pros and cons, but many would work out just fine.</p>

<p>For example, you could have a <code>Job</code> model with these kinds of states: <code>invited</code>, <code>invitation_declined</code>, <code>assigned</code>, <code>appointment_made</code>, <code>invoiced</code>, <code>invoice_paid</code>, <code>canceled</code>, etc. Each would only allow the valid transitions as described above. You would also need the columns to represent the data: <code>client_id</code>, <code>tasker_id</code>, <code>appointment_at</code>, etc.</p>

<p>The main benefit of this approach is centrality. You can <code>SELECT * FROM jobs WHERE client_id = 42</code> and get all of that user's situation. Over time, however, we came to value a more decentralized approach.</p>

<p>Now, the models of our system reflect its objects and decisions that the actors make about them. Each fork in the experience has a corresponding model with a simple state machine.</p>

<p>For example, the <code>Invitation</code> model is created first to note the decision the Tasker must make. It then either transitions to <code>accepted</code> or <code>declined</code>.  If accepted, it spawns an <code>Assignment</code>. It, in turn, can move to states like <code>completed</code> or <code>ongoing</code>.</p>

<div class="jumbotron">
<img src="http://bleonard.github.io/images/posts/architecture-models/invitations.png" class="bigPicture" />
</div>


<p>There is still the the <code>Job</code> model but it contains the "description" of the work to do and its <code>id</code> ties together the decision-based models.</p>

<h3>Trade-offs</h3>

<p>Everything is pros and cons. The decentralized approach has more global complexity (more objects and interactions) but less local complexity (simpler decisions, states).</p>

<p>It seemed to be the single, monolithic state machine that doomed the single <code>Job</code> model. Everything is fine as long as that's the only path through the system. However, as soon as there is a new way for a Task to be assigned, we have a tangled web of states.</p>

<p>Not every task has the invitation pattern noted above. Some are "broadcast" to many Taskers at once and shown in a browse-able "Available Tasks" section in the their app. That's a new fork in the experience. Ongoing tasks also create a state loop of sorts.</p>

<p>These cause the single state machine to get a bit tangled up, but is more easily handled in the decentralized approach. We can make a <code>Broadcast</code> model instead of an <code>Invitation</code> one. That can have its own set of states. Success in that local state machine can also spawn an <code>Assignment</code> and everything goes on as before.</p>

<h3>Denormalization</h3>

<p>To try and get the best of both worlds, we have also aggressively embraced a variety of forms of denormalization.</p>

<p>We actively try not to do SQL <code>JOIN</code>s for simplicity and performance reasons, but that is at odds with all these little models all over the place. So we have said it's OK to have duplicate data. For example, each of these "decision" models have the <code>client_id</code>, <code>tasker_id</code>, and pricing information. It just gets passed along. This makes everything a local decision and queries very straightforward.</p>

<p>The big hole in the decentralized approach is to "get all my stuff" easily. For that we have different tactics, both of which are denormalization with use cases in mind.</p>

<p>On write to an object, we can update a central model with the current situation for that <code>Job</code>. For example, when an <code>Assignment</code> gets created, we recalculate and store data in two different tables. One for both the Tasker and the Client on what they should be seeing on their respective dashboards. Thus, the API call to "get all my stuff" uses one of those tables. That is done in the same transaction as the original write.</p>

<p>The other option is basically the same thing but for either less time-sensitive data or more complicated queries. We use a <a href="/blog/2015/04/02/queue-bus/">message bus</a> to observe changes. We then denormalize applicable data for a specific use case into a table or <a href="http://www.elastic.co">Elasticsearch</a>. For example, when an <code>Appointment</code> is created, we would update the Taskers availability schedule in the database. Updating this schedule would also trigger an update to our recommendation algorithm which uses Elasticsearch.</p>

<p>One important note: all of these denormalizations should be <a href="http://www.restapitutorial.com/lessons/idempotency.html">idempotent</a>. This allows us to recreate the whole thing from the source of truth or recover if any given event is dropped.</p>

<h3>Summary</h3>

<p>At TaskRabbit, we default to the following rules when designing the models in a system:</p>

<ul>
<li>Keep the scope small and based on decisions in the workflow</li>
<li>Use state machines to declare and lock in the valid transitions</li>
<li>Denormalize as needed to optimize use cases in the experience</li>
</ul>


<p>As always, these are just the default guidelines. In any given case, there may be a reason to deviate, but it would have to be clear why that case was special.</p>
]]></content>
  </entry>
  
</feed>
