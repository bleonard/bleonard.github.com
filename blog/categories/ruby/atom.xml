<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ruby | BLog]]></title>
  <link href="http://bleonard.github.io/blog/categories/ruby/atom.xml" rel="self"/>
  <link href="http://bleonard.github.io/"/>
  <updated>2017-04-14T15:13:13-07:00</updated>
  <id>http://bleonard.github.io/</id>
  <author>
    <name><![CDATA[Brian Leonard]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Big Decimal]]></title>
    <link href="http://bleonard.github.io/blog/2015/06/04/big-decimal/"/>
    <updated>2015-06-04T00:00:00-07:00</updated>
    <id>http://bleonard.github.io/blog/2015/06/04/big-decimal</id>
    <content type="html"><![CDATA[<p>At <a href="http://www.taskrabbit.com">TaskRabbit</a>, people have hourly rates for the tasks that they do. They get paid based on the number of hours that they work.</p>

<p>We try to deal in base units to remove ambiguity. For example, we store the hourly rate in cents as an integer instead of in dollars as a double. Reporting time worked occurs in seconds. When that reporting occurs, there is a calculation to do to understand how much the Tasker needs to be paid. It might go something like this:</p>

<p>```ruby
class FloatCalculator
  def initialize(hourly_rate_in_cents)</p>

<pre><code>@hourly_rate_in_cents = hourly_rate_in_cents
</code></pre>

<p>  end</p>

<p>  def total_in_dollars(seconds_worked)</p>

<pre><code>hours = seconds_worked / 3600.0 # seconds in an hour
cents = @hourly_rate_in_cents * hours
dollars = cents / 100 # make it dollars
dollars.round(2)
</code></pre>

<p>  end
end
```</p>

<p>Over the years, we saw a few errors because of <a href="http://floating-point-gui.de/">limitations of floating point math</a>. Since then, we have switched to using <a href="http://ruby-doc.org/stdlib-2.1.5/libdoc/bigdecimal/rdoc/BigDecimal.html">BigDecimal</a> to prevent these kinds of errors.</p>

<p>There were no visible performance issues, but I recently got curious because all of the docs say that they are substantial. By writing the same calculator using BigDecimal, we can see the difference.</p>

<p>```ruby
class BigDecimalCalculator
  def initialize(hourly_rate_in_cents)</p>

<pre><code>@hourly_rate_in_cents = BigDecimal.new(hourly_rate_in_cents)
</code></pre>

<p>  end</p>

<p>  def total_in_dollars(seconds_worked)</p>

<pre><code>hours = BigDecimal.new(seconds_worked) / BigDecimal.new(3600)
cents = @hourly_rate_in_cents * BigDecimal.new(hours)
dollars = cents / BigDecimal.new(100)
dollars.round(2).to_f
</code></pre>

<p>  end
end
```</p>

<p>This does turn out to be significantly slower, so I also benchmarked marking the constants class level variables.</p>

<!-- more -->


<p>```ruby
class BigDecimalStaticCalculator
  CONVERSION_RATE = BigDecimal.new(3600) * BigDecimal.new(100)</p>

<p>  def initialize(hourly_rate_in_cents)</p>

<pre><code>@hourly_rate_in_cents = BigDecimal.new(hourly_rate_in_cents)
</code></pre>

<p>  end</p>

<p>  def total_in_dollars(seconds_worked)</p>

<pre><code>dollars = @hourly_rate_in_cents * BigDecimal.new(seconds_worked) / CONVERSION_RATE
dollars.round(2).to_f
</code></pre>

<p>  end
end
```</p>

<h3>Results</h3>

<p>The results are in!</p>

<p>```</p>

<pre><code>                     user     system      total        real
</code></pre>

<p>floats               0.220000   0.010000   0.230000 (  0.231120)
big decimal          4.130000   0.070000   4.200000 (  4.234744)
big decimal static   2.620000   0.040000   2.660000 (  2.680582)
```</p>

<p>The cost of using BigDecimal to reduce the error rate is a slowdown of 15x-20x compared to floating point math. This is not a big deal in practice. First of all, we need to get the money correct. Secondly, these are times for 250,000 calculations. Any given request only has 1, which is nothing compared to the other stuff (SQL, template rendering) happening.</p>

<p>It seems that the initialization process takes up a lot of time, though, so one technique to speed it up is to pull out constants in your BigDecimal code to the class level. These would only be initialized once, but may also affect the base memory level.</p>

<p>These classes and the benchmark itself are available as a <a href="https://gist.github.com/bleonard/6d7ffa979e9720baf71b">gist</a> if you are interested.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Rails Dead Columns]]></title>
    <link href="http://bleonard.github.io/blog/2014/11/13/rails-dead-columns/"/>
    <updated>2014-11-13T00:00:00-08:00</updated>
    <id>http://bleonard.github.io/blog/2014/11/13/rails-dead-columns</id>
    <content type="html"><![CDATA[<p>It happens. This column has to go. But in a <a href="http://rubyonrails.org/">Rails</a> app, there are a few problems. You can't just drop it.</p>

<p>My first expectation is that we would create a <a href="http://guides.rubyonrails.org/migrations.html">migration</a> and do something like this.</p>

<p>```
class RemoveMiddleNameFromUsers &lt; ::ActiveRecord::Migration
  def change</p>

<pre><code>remove_column :users, :middle_name
remove_column :users, :gender
</code></pre>

<p>  end
end
```</p>

<p>This will generally be fine but there are a few practical issues.</p>

<ul>
<li>Is any of my code still using this column?</li>
<li>What happens to the production code in the time between the migration and new code using it?</li>
</ul>


<h2>Still in use?</h2>

<p>The obvious thing to do is search the code for use of this column name, but sometimes that can be tricky. The name would be fairly common leading to difficult searching. You could also be accessing it in a fairly inconsistent way like via <code>send</code> and string interpolation or something.</p>

<p>The best way that we've found out that we are still using something is to freak out in test/development/staging if it is accessed. If we didn't have the test coverage, we'd probably log it's usage in production and check the logs for use.</p>

<!-- more -->


<h2>Deploy</h2>

<p>Even if you aren't using the column, there's still an issue during the deploy window after the migration and before the server reboots. Rails has cached all the columns from right after it booted up so that it can know what to write to the table when you save a new record (among other things).</p>

<p>This means that it still assumes that column is there and when it reads/writes it will look for or set that column. Obviously this will not work. So what we need is two deploys wherein we first tell ActiveRecord that column no longer exists. Then sometime in the future, we can actually remove it.</p>

<p>Incidentally, I've noticed a read-only (<code>def readonly?</code> returns <code>true</code>) ActiveRecord model always does a <code>SELECT *</code> instead of looking for specific columns and doesn't have this issue.</p>

<h2>Code</h2>

<p>Here is the code use for these purposes.</p>

<p>```ruby</p>

<p>module Mixins
  module DeadColumns</p>

<pre><code>extend ActiveSupport::Concern

class DeadColumnError &lt; StandardError; end

# to run in config/initializers to remove things globally
class All
  class &lt;&lt; self
    def init
      return if @active_record_init
      @active_record_init = true
      ActiveRecord::Base.send(:include, Mixins::DeadColumns)
    end

    def death_watch(name, *columns)
      init
      @registered_tables ||= {}
      @registered_tables[name.to_s] = columns.collect(&amp;:to_s)
    end

    def table_columns_registered(name)
      @registered_tables[name.to_s] || []
    end
  end
end

included do
  class &lt;&lt; self
    alias_method_chain :columns, :death
  end
end

module ClassMethods

  def dead_column_list
    return @dead_columns if @dead_columns
    @dead_columns = Mixins::DeadColumns::All.table_columns_registered(self.table_name).dup
  end

  def columns_with_death
    @columns_with_death ||= columns_without_death.reject{|c| dead_column_list.include?(c.name.to_s) }
  end

  def dead_columns(*args)
    to_kill = args.collect(&amp;:to_s)
    dead_column_list.concat(to_kill)

    to_kill.each do |col|

      define_method("#{col}") do
        raise DeadColumnError, "#{self.class.name}##{col}"
      end

      define_method("#{col}?") do
        raise DeadColumnError, "#{self.class.name}##{col}?"
      end

      define_method("#{col}_changed?") do
        raise DeadColumnError, "#{self.class.name}##{col}?"
      end
    end

    # reset all these ActiveRecord caches
    @dynamic_methods_hash = @columns_hash = @column_names = @content_columns = @column_defaults = @columns = @columns_with_death = nil
  end
end

def attribute_names
  super - self.class.dead_column_list
end

def as_json(options = nil)
  options ||= {}
  options[:only] = self.class.column_names if options[:only].nil?
  super(options)
end
</code></pre>

<p>  end
end
```</p>

<p>Then in an initializer, we do this:</p>

<p>```ruby</p>

<h1>config/dead_columns.rb</h1>

<p>Mixins::DeadColumns::All.death_watch(:users, :middle_name, :gender)
Mixins::DeadColumns::All.death_watch(:tasks, :old_price_info)
```</p>

<p>We used to mix this in to each model, so it was more like this:</p>

<p>```ruby
class User &lt; ActiveRecord::Base
  include Mixins::DeadColumns</p>

<p>  dead_columns :middle_name, ::gender
end
```</p>

<p>This worked with a slightly modified version of the above code. We stopped using that version, though, when we switched to heavily using <a href="/blog/2014/02/11/rails-4-engines/">engines</a>, and now have many <code>User</code> (and other) models. The initializer works better to make sure it hits all of them.</p>

<h2>Conclusion</h2>

<p>Hopefully, this is helpful as an example of some code to deploy so you can safely remove your columns. It feels good to drop those columns and now you can do it without the pain.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Geohashes]]></title>
    <link href="http://bleonard.github.io/blog/2014/09/24/geohashes/"/>
    <updated>2014-09-24T00:00:00-07:00</updated>
    <id>http://bleonard.github.io/blog/2014/09/24/geohashes</id>
    <content type="html"><![CDATA[<p>Over the years, we've struggled at <a href="https://www.taskrabbit.com">TaskRabbit</a> with a way to represent smaller geographic areas. Postal codes seems to be the most obvious but these vary dramatically in size. For example, there are postal codes that are just one building and ones that are bigger than some US states. We've also tried "neighborhood" data from various sources, but these are somewhat unreliable.</p>

<p>I recently learned about and started using <a href="http://en.wikipedia.org/wiki/Geohash">geohashes</a> to break the world into boxes and have been very happy with the results. I gave a lightning talk last week at <a href="http://gogaruco.com/">GoGaRuCo</a> on the subject. Many of them had heard about it, but the talk went over fairly well.</p>

<h3>Use Case</h3>

<p>The problem at hand is one of probabilities. Given a task, who are the best taskers for the job. Or conversely, given a tasker, what are the best few tasks for them. Taskers give us an idea of where they want to work but actions often speak louder than <s>words</s> user-drawn geographic polygons. So what I wanted to do was map their history into positive and negative areas of probability depending on what tasks they wanted to do and which they did not.</p>

<p>I had all those points and it was fairly easy to draw a heatmap using <a href="https://www.mapbox.com/mapbox.js/example/v1.0.0/leaflet-heat/">this Leaflet plugin</a>, but that needed to be mapped to something more concrete. We would be using <a href="http://www.elasticsearch.org/">Elasticsearch</a> for the storage, so while researching it's geo functions, I stumbled across the concept of <a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-geohashgrid-aggregation.html">geohashes</a> which it evidently uses internally for such queries.</p>

<p><a href="http://www.bigfastblog.com/geohash-intro">This</a> article gives a really good explanation of how it works. The short version is that if you were to start with the whole world and keep cutting it in half depending on where your lat/lng was, you'd end up with a lot of left/right (or top/bottom) choices. Those choices can be encoded into binary, which can be encoded into a simple string. So this spot in London (51.507794, -0.127952) would become the "gcpvj0dyds" geohash. It has the interesting property that strings (locations) that have the same start are known to be close to it. For example, the "gcpvj09swr" geohash has the first six characters in common, so we would know that is within half a mile or so of the first one.</p>

<p>So now I have all of these points. I can map them to geohashes of the necessary precision (I chose 6) and give positive and negative weights for each user action. The actual calculation turns out to be really fast. I used the code from the <a href="https://github.com/masuidrive/pr_geohash/blob/master/lib/pr_geohash.rb">pr_geohash</a> gem and had no issues. It also calculates neighbors which turns out the be important.</p>

<p>Using the weight, I can combine and draw the geohashes (<a href="https://github.com/rgeo/rgeo">RGeo</a>/<a href="http://leafletjs.com/">Leaflet</a>). As an example, here is drawing where a particular Tasker is likely to want to work:</p>

<p><img src="/images/posts/geohashes/medium.jpg" alt="Likely to Accept" /></p>

<!-- more -->


<p>And where this Tasker is generally positive:</p>

<p><img src="/images/posts/geohashes/low.jpg" alt="Positive" /></p>

<p>And also where they have negative feedback:</p>

<p><img src="/images/posts/geohashes/negative.jpg" alt="Negative" /></p>

<p>Using these tools and the fact that every Tasker's map is different, we can make better recommendations. This reduces friction in the marketplace.</p>

<p>One of the gotchas is mentioned in the <a href="http://www.bigfastblog.com/geohash-intro">article</a> I referenced earlier. If a point is on a border, especially at the beginning of the decision process, it can end up being massively off. I worked around this in my implementation by spreading out the weight to it's neighbors. So if the location got 100 points, the neighbors would also get some, degrading as it spread out.</p>

<p><img src="/images/posts/geohashes/boxes.jpg" alt="Spread out the weight" /></p>

<p>This has proven to minimize the boundary issue.</p>

<h3>Conclusion</h3>

<p>Overall, I just wanted to share a new tool and concept that I learned about. We've found it to be a great pattern and will likely use geohashes more for representing our world.</p>
]]></content>
  </entry>
  
</feed>
