<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: resque | BLog]]></title>
  <link href="http://bleonard.github.io/blog/categories/resque/atom.xml" rel="self"/>
  <link href="http://bleonard.github.io/"/>
  <updated>2017-04-14T15:13:13-07:00</updated>
  <id>http://bleonard.github.io/</id>
  <author>
    <name><![CDATA[Brian Leonard]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Architecture: Consider Kron]]></title>
    <link href="http://bleonard.github.io/blog/2017/04/14/architecture-consider-kron/"/>
    <updated>2017-04-14T00:00:00-07:00</updated>
    <id>http://bleonard.github.io/blog/2017/04/14/architecture-consider-kron</id>
    <content type="html"><![CDATA[<p>The last post in our architecture series discussed <a href="/blog/2017/03/17/architecture-background-processing/">background processing</a>. There is a special type of background processing that I wanted to make a quick note about. These are things that need to be done periodically or otherwise on a schedule.</p>

<p>In our internal speak, we call this a "kron" job. If you are familiar with <a href="https://en.wikipedia.org/wiki/Cron">cron</a> jobs, it's the same idea. A product manager misspelled it once and it stuck! We don't actually use regular cron infrastructure, so the spelling <em>nuance</em> is helpful.</p>

<p>The specifics of how we implement it involve our <a href="/blog/2015/04/02/queue-bus/">message bus</a> infrastructure, but I think the concept and the decisions involved could include many other implementations.</p>

<h3>When to use it</h3>

<p>Let's take the job from the <a href="/blog/2017/03/17/architecture-background-processing/">previous article</a>. The "charge an invoice 24 hours later" case is an interesting one. The system certainly supports delaying that code to run for an arbitrary time, but that's not always the best idea.</p>

<p>```ruby
class InvoiceChargeWorker
  include TResque::Worker
  inputs :invoice_id</p>

<p>  worker_lock :invoice_id</p>

<p>  def work</p>

<pre><code>return unless needed?
invoice.charge!
</code></pre>

<p>  end</p>

<p>  def to_id</p>

<pre><code>invoice.to_id
</code></pre>

<p>  end</p>

<p>  def needed?</p>

<pre><code>invoice.pending?
</code></pre>

<p>  end</p>

<p>  def invoice</p>

<pre><code>@invoice ||= Invoice.find(invoice_id)
</code></pre>

<p>  end
end</p>

<h1>When invoice is created</h1>

<p>InvoiceChargeWorker.enqueue_at(24.hours.from_now, invoice_id: invoice.id)
```</p>

<p>One reason would be memory. When there a lot of invoices (woot!), we still have to save the notion of what should be done somewhere until it gets processed. In this case, the <a href="https://redis.io/">Redis</a> instance will have it stored in memory. The memory could fill up and adding more workers won't help because of the delay.</p>

<p>The second reason is stability. This is important stuff and Redis could have issues and lose the data. We made everything <a href="http://www.restapitutorial.com/lessons/idempotency.html">idempotent</a> and could recreate everything, but it would certainly be a huge hassle.</p>

<p>So when enqueueing something to run in the future, especially if it is important or a long time from now (more than a few minutes), we consider kron.</p>

<h3>Batch mode</h3>

<p>If we were going to accomplish the same things but on a schedule, the code would have to change in some way. I like the existing worker because it already has the good stuff from the last article: source of truth, knowing whether or not it still needs to be run, and mutual exclusion. When batch processing, I believe it's also good to still operate on this one at a time where the count (memory for redis) is low or the risk of issues is high. Both are the case here.</p>

<p>To turn it into a batch processor we need to know what needs to be processed at any given moment. This is easy to determine because we have the <code>needed?</code> method. It looks to be invoices that are in the <code>pending</code> state. Sometimes we need to add a <code>state</code> column or other piece of data to know what needs to be in the batch but in this case we are good to go.</p>

<p>From there we can decide if we are going to update the class as-is or make a batch worker. A batch worker is its own worker and would look like this:</p>

<p>```ruby
class InvoiceChargeBatchWorker
  include TResque::Worker</p>

<p>  worker_lock :all
  queue_lock  :all</p>

<p>  def work</p>

<pre><code>Invoice.where(stat: 'pending').find_each do |invoice|
  InvoiceChargeWorker.enqueue(invoice_id: invoice.id)
end
</code></pre>

<p>  end
end</p>

<h1>process all pending invoices</h1>

<p>InvoiceChargeBatchWorker.enqueue()
```</p>

<p>That's it. Because of the worker lock on <code>InvoiceChargeWorker</code> and the state checking, it would be ok even if we were to enqueue it twice or something. Making a custom batch worker also prevents us from running this code twice.</p>

<p>We could also stick it as a class method on the original:</p>

<p>```ruby
class InvoiceChargeWorker
  include TResque::Worker
  inputs :invoice_id</p>

<p>  worker_lock :invoice_id</p>

<p>  def self.process_all!</p>

<pre><code>Invoice.where(stat: 'pending').find_each do |invoice|
  self.enqueue(invoice_id: invoice.id)
end
</code></pre>

<p>  end</p>

<p>  def work</p>

<pre><code>return unless needed?
invoice.charge!
</code></pre>

<p>  end</p>

<p>  def needed?</p>

<pre><code>invoice.pending?
</code></pre>

<p>  end</p>

<p>  def invoice</p>

<pre><code>@invoice ||= Invoice.find(invoice_id)
</code></pre>

<p>  end
end</p>

<h1>process all pending invoices</h1>

<p>InvoiceChargeWorker.process_all!
```</p>

<h3>How it works</h3>

<p>Again, in any given architecture there is probably a best way to do it. For example, maybe <a href="https://medium.com/airbnb-engineering/chronos-a-replacement-for-cron-f05d7d986a9d">this</a> is a good way to do it on top of <a href="http://mesos.apache.org/">Mesos</a>.</p>

<p>The challenge is running something on a schedule. In this case, process all invoices that need to be paid. That is what regular cron is made to do. However, we do not want to run that on every box. If we did that, we would have serious race conditions and might pay an invoice twice. Rather, we want to run it once globally across the entire infrastructure or at least per service.</p>

<p>We could probably do this by noting in the devops setup that one of the servers is special. It should get the cron setup. We could use something like the <a href="https://github.com/javan/whenever">whenever gem</a> to say what to do and we would only run that on one box per system. It needs to be per system because it has to be able to know what worker to enqueue or, in general, what code to run.</p>

<p>What we do instead is have a single service that has a process that sends out a heartbeat on the <a href="/blog/2015/04/02/queue-bus/">message bus</a>. Every minute, it publishes an event that looks like this.</p>

<p>```ruby
  # for Tue, 11 Apr 2017 00:25:00 UTC +00:00
  # epoch time: 1491870300</p>

<p>  QueueBus.publish(heartbeat_seconds", {</p>

<pre><code>"epoch_seconds"=&gt;1491870300,
"epoch_minutes"=&gt;24864505,
"epoch_hours"=&gt;414408,
"epoch_days"=&gt;17267,
"minute"=&gt;25,
"hour"=&gt;0, 
"day"=&gt;11,
"month"=&gt;4,
"year"=&gt;2017,
"yday"=&gt;101,
"wday"=&gt;2
</code></pre>

<p>  })
```</p>

<p>The current code for the process is already checked into <a href="https://github.com/queue-bus/queue-bus">queue-bus</a> and ready to use <a href="https://github.com/queue-bus/queue-bus/blob/5c009a3b2d2d58994813bdca0dc78547a18b0295/lib/queue_bus/heartbeat.rb">here</a>.</p>

<p><a href="https://github.com/queue-bus/resque-bus">Resque bus</a> supports this using the <a href="https://github.com/resque/resque-scheduler">resque-scheduler gem</a>. It is setup off by calling <code>QueueBus.heartbeat!</code>. We make sure it's setup every time we start up Resque.</p>

<p>```ruby
namespace :resque do
  task :setup => [:environment] do</p>

<pre><code>require 'resque_scheduler'
require 'resque/scheduler'
require 'tresque'

QueueBus.heartbeat!
</code></pre>

<p>  end
end
```</p>

<p>This <code>setup</code> is automatically called every time Resque starts.</p>

<h3>Usage</h3>

<p>So now we can subscribe to this event to run something every minute, hour, day, Monday, month, whatever.</p>

<p>```ruby</p>

<h1>every minute</h1>

<p>subscribe "every_minute", 'bus_event_type' => 'heartbeat_minutes' do |attributes|
  InvoiceChargeWorker.process_all!
end</p>

<h1>every hour: 4:22, 5:22, 6:22, etc</h1>

<p>subscribe "once_an_hour", 'bus_event_type' => 'heartbeat_minutes', 'minute' => 22 do |attributes|
  InvoiceChargeWorker.process_all!
end</p>

<h1>every day at 12:05 am</h1>

<p>subscribe "once_a_day", 'bus_event_type' => 'heartbeat_minutes', 'hour' => 0, 'minute' => 5 do |attributes|
  InvoiceChargeWorker.process_all!
end</p>

<h1>every monday at 1:52 am</h1>

<p>subscribe "early_monday_morning", 'bus_event_type' => 'heartbeat_minutes', 'wday' => 1, 'hour' => 1, 'minute' => 52 do |attributes|
  InvoiceChargeWorker.process_all!
end</p>

<h1>the 3rd of every month at 2:10 am</h1>

<p>subscribe "once_a_month", 'bus_event_type' => 'heartbeat_minutes', 'day' => 3, 'hour' => 2, 'minute' => 10 do |attributes|
  InvoiceChargeWorker.process_all!
end</p>

<h1>every 5 minutes: 4:00, 4:05, 4:10, etc</h1>

<p>subscribe "every 5 minutes" do |attributes|
  # if it doesn't fit the subscribe pattern, just subscribe to every minute and use ruby
  next unless attributes['minute'] % 5 == 0
  InvoiceChargeWorker.process_all!
end</p>

<p>```</p>

<h3>Summary</h3>

<p>So that is how "kron" works.</p>

<p>Over time, we have decided this is a much more reliable way to process items in the background when a delay is acceptable. By setting up some sort of centralized architecture for this, many services and subscribe in a way that is familiar and unsurprising. We have found a lot of value in that.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Architecture: Background Processing]]></title>
    <link href="http://bleonard.github.io/blog/2017/03/17/architecture-background-processing/"/>
    <updated>2017-03-17T00:00:00-07:00</updated>
    <id>http://bleonard.github.io/blog/2017/03/17/architecture-background-processing</id>
    <content type="html"><![CDATA[<p>So we have a bunch of <a href="/blog/2017/02/24/architecture-models/">models</a> and are doing stuff with them in <a href="/blog/2017/03/03/architecture-service-objects/">service objects</a>. The next thing we might need is to process some code in the background.</p>

<p>Not everything can be done inline from the API request. For example, we might need to geocode a user's postal code when they change it in their account. Or when an invoice is created, we want to charge it 24 hours later.</p>

<p>When working with background jobs, we default to the following practices:</p>

<ul>
<li>Workers are enqueued with a dictionary of inputs</li>
<li>These inputs should be used to fetch data from the source of truth</li>
<li>Workers know how to check if they still need to run</li>
<li>Locking schemes should protect parallel execution</li>
</ul>


<h3>Enqueue</h3>

<p>When we enqueue a worker, we have found that it's quite helpful to always use a dictionary (hash) of key/value pairs. <a href="https://github.com/resque/resque">Resque</a> and <a href="http://sidekiq.org/">Sidekiq</a> both take a list of arguments like <a href="https://github.com/mperham/sidekiq/wiki/Getting-Started">so</a>:</p>

<p>```ruby
class HardWorker
  include Sidekiq::Worker
  def perform(name, count)</p>

<pre><code># do something with name, count
</code></pre>

<p>  end
end</p>

<h1>enqueue</h1>

<p>HardWorker.perform_async('bob', 5)
```</p>

<p>This has proved to be problematic when adding new parameters or having optional parameters. For example, if we add a new (third) input parameter, there might be stuff in the queue with the old two. When the new code gets deployed, it will throw an 'invalid number of arguments' type of error. When using a hash, we can give it a default, fail gracefully, or do whatever we like on a class by class basis.</p>

<p>So to provide better change management and optional arguments, we always do it like so:</p>

<p>```ruby
class HardWorker
  include TResque::Worker
  inputs :name, :count</p>

<p>  def work</p>

<pre><code># do something with self.name, self.count
</code></pre>

<p>  end
end</p>

<h1>enqueue</h1>

<p>HardWorker.enqueue(name: 'bob', count: 5)
```</p>

<h3>Source of Truth</h3>

<p>Let's say we want to update a search index every time a user record is changed. We need to write their first name, last name, etc to <a href="https://www.elastic.co/">Elasticsearch</a>.</p>

<p>We could do something like this:</p>

<p>```ruby
class UserIndexWorker
  include TResque::Worker
  inputs :id, :first_name, :last_name, :etc</p>

<p>  def work</p>

<pre><code>Elasticsearch.index('users').write(id, id: id, first_name: first_name, last_name: last_name, etc: etc)
</code></pre>

<p>  end
end</p>

<h1>When user changes</h1>

<p>UserIndexWorker.enqueue(user.attributes.slice(:id, :first_name, :last_name, :etc))
```</p>

<p>This certainly would work, but is not considered best practice. It is better to be <a href="http://www.restapitutorial.com/lessons/idempotency.html">idempotent</a>. It writes everything that should ) by passing the minimal information to the background worker, who then looks up the source of truth. That way, if there is any delay between when it is enqueued and run, it will still send the correct information.</p>

<p>The better approach would look like this:</p>

<p>```ruby
class UserIndexWorker
  include TResque::Worker
  inputs :user_id</p>

<p>  def work</p>

<pre><code>Elasticsearch.index('users').write(user.attributes.slice(:id, :first_name, :last_name, :etc))
</code></pre>

<p>  end</p>

<p>  def user</p>

<pre><code>@user ||= User.find(user_id)
</code></pre>

<p>  end
end</p>

<h1>When user changes</h1>

<p>UserIndexWorker.enqueue(user_id: user.id)
```</p>

<p>In the same vein, the worker should be in charge of whether or not it needs to do anything in the first place. For example, we can enqueue a worker to run later about an <code>Invoice</code>. If, at that time, the payment is <code>Invoice</code> still should be charged, then charge it.</p>

<p>```ruby
class InvoiceChargeWorker
  include TResque::Worker
  inputs :invoice_id</p>

<p>  def work</p>

<pre><code>return unless needed?
invoice.charge!
</code></pre>

<p>  end</p>

<p>  def needed?</p>

<pre><code>invoice.pending?
</code></pre>

<p>  end</p>

<p>  def invoice</p>

<pre><code>@invoice ||= Invoice.find(invoice_id)
</code></pre>

<p>  end
end</p>

<h1>When invoice is created</h1>

<p>InvoiceChargeWorker.enqueue_at(24.hours.from_now, invoice_id: invoice.id)
```</p>

<p>This is another example of single source of truth. Even for jobs that are run immediately, this check is something we always put in place: return immediately if the worker is no longer relevant.</p>

<h3>Mutual Exclusion</h3>

<p>Let's say the <code>User</code> object can sometimes change a few times rapidly. The "source of truth" approach will make sure the right thing always gets indexed. So that's great. But it is pretty silly to index the same data twice or more times, right?</p>

<p>In this case, we add a queue lock. The effect is that if something is in the queue and waiting to be processed and you try to enqueue another one with the same inputs, then it will be a no-op. It looks like this:</p>

<p>```ruby
class UserIndexWorker
  include TResque::Worker
  inputs :user_id</p>

<p>  queue_lock :user_id
end
```</p>

<p>Another case that often arises is mutual exclusion for <em>runtime</em>. Maybe weird payment things happen to the payment service if two invoices for the same user are happening at the same time.</p>

<p>In this case, we add a worker lock. The effect is that if something is in the queue and about to start running and there is another running at that moment, then it will re-enqueue itself to run later. It looks like this:</p>

<p>```ruby
class InvoiceChargeWorker
  include TResque::Worker
  inputs :invoice_id</p>

<p>  worker_lock :to_id</p>

<p>  def work</p>

<pre><code>return unless needed?
invoice.charge!
</code></pre>

<p>  end</p>

<p>  def to_id</p>

<pre><code>invoice.to_id
</code></pre>

<p>  end</p>

<p>  def needed?</p>

<pre><code>invoice.pending?
</code></pre>

<p>  end</p>

<p>  def invoice</p>

<pre><code>@invoice ||= Invoice.find(invoice_id)
</code></pre>

<p>  end
end
```</p>

<p>For either type, you don't have to lock on all the attributes or can (as shown in the last example) use calculations. The namespace of the lock is the worker class name. You can also set the namespace to allow locking between different workers.</p>

<h3>Message Bus</h3>

<p>Our <a href="/blog/2015/04/02/queue-bus/">message bus</a> and our use of background processes have a lot in common. In fact, the message bus is built on top of the same background processing infrastructure. The question that arises is this: when should something be enqueued directly and when should it publish and respond to a bus subscription?</p>

<p>The first note is that you should <em>always be publishing</em> (ABP). It doesn't hurt anything to give (optional) visibility to other systems what is happening. Or use this as logging framework.</p>

<p>Just publishing, however, doesn't mean we have to use that to do work in the background. Be can bother publish and enqueue a background worker. We enqueue a worker when the work in the background is essential to the correct operation of the use case at hand.</p>

<p>One example to enqueue directly would be the geocoding worker I mentioned earlier: when the user gives a new postal code, figure out where that is. It's key to the account management system.</p>

<p>The search example I've been using might not actually be the best one because we would have the search system subscribed to changes in the account system. What I didn't show that the <code>enqueue</code> call might actually happen from within a subscription.</p>

<p><code>ruby
subscribe "user_changed" do |attributes|
  UserIndexWorker.enqueue(user_id: attributes['id'])
end
</code></p>

<p>So these two concepts can work together. Why not just index it right in the subscription, though? A primary reason might be to use some of the locking mechanisms as the bus does not have that. It also might be the case that the worker is enqueued from other locations and this keeps things DRY. The worker is also easier to unit test.</p>

<h3>TResque</h3>

<p>We use <a href="https://github.com/resque/resque">Resque</a> as a base foundation and built on top of it with an abstraction layer called <a href="https://github.com/taskrabbit/tresque">TResque</a>. That's TR (TaskRabbit) Resque. Get it? It puts all of these practices into place as well as adding and abstraction layer for the inevitable, but as yet unprioritized, move to <a href="http://sidekiq.org/">Sidekiq</a>.</p>

<p>I don't necessarily expect anyone to use this, but it doesn't hurt to make it available as an example of how we are using these tools.</p>

<p>You define a worker and enqueue things as show in the examples above. Then only layer left is around prioritization. You can give a queue name to a worker and then register what priority those workers are. If no queue is given, it is assumed to be the <code>default</code> queue.</p>

<p>```ruby
require 'tresque'</p>

<p>module Account
  class RegularWorker</p>

<pre><code>include ::TResque::Worker
# defaults to account_default queue
</code></pre>

<p>  end
end</p>

<p>module Account
  class RegularWorker</p>

<pre><code>include ::TResque::Worker
queue :refresh # lower priority account_refresh queue
</code></pre>

<p>  end
end</p>

<p>TResque.register("account") do
  queue :default, 100
  queue :refresh, -5000
end
```</p>

<p>Then when you run Resque, you can use these registrations to process the queues in the right order.</p>

<p>```ruby
require 'resque/tasks'
require 'resque_scheduler/tasks'
require "resque_bus/tasks"</p>

<p>namespace :resque do
  task :setup => [:environment] do</p>

<pre><code>require 'resque_scheduler'
require 'resque/scheduler'
require 'tresque'
</code></pre>

<p>  end</p>

<p>  task :queues => [:setup] do</p>

<pre><code>queues = ::TResque::Registry.queues
ENV["QUEUES"] = queues.join(",")
puts "TResque: #{ENV["QUEUES"]}"
</code></pre>

<p>  end
end</p>

<p>```</p>

<p><code>
  $ bundle exec rake resque:queues resque:work
  TResque: account_default, account_refresh
</code></p>

<p>This registration layer allows each of the systems (<a href="/blog/2014/02/11/rails-4-engines/">engines</a>) to work independently and still have centralized background processing.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Queue Bus]]></title>
    <link href="http://bleonard.github.io/blog/2015/04/02/queue-bus/"/>
    <updated>2015-04-02T00:00:00-07:00</updated>
    <id>http://bleonard.github.io/blog/2015/04/02/queue-bus</id>
    <content type="html"><![CDATA[<p>At <a href="http://www.taskrabbit.com">TaskRabbit</a>, we have been using <a href="/blog/2013/09/28/resque-bus/">resque-bus</a> for about two years. It has continued to provide value by  linking components via a loosely coupled publish/subscription model. We have seen 10x the number of events going through it, but have not yet hit any scaling issues. The benefit of using the tools we already have continues to be a huge win.</p>

<p>But other teams are using other tools like <a href="http://sidekiq.org/">Sidekiq</a>. We're also interested in trying it out, but resque-bus (unsurprisingly) was tied closely to <a href="https://github.com/resque/resque">Resque</a>. We've changed that by creating <a href="https://github.com/queue-bus/queue-bus">queue-bus</a> and using an adapter pattern. There are now adapters in <a href="https://github.com/queue-bus/resque-bus">resque-bus</a> and <a href="https://github.com/queue-bus/sidekiq-bus">sidekiq-bus</a>, as well as a compatible version in <a href="https://github.com/queue-bus/node-queue-bus">node-queue-bus</a>.</p>

<p>The code interactions are basically the same but can work across systems. You can even have one app using Resque and one using Sidekiq.</p>

<h4>Pick your adapter</h4>

<p>By requiring one of the adapters, it automatically gets set up to be the one for the app.</p>

<p><code>ruby
require 'sidekiq-bus' # (or 'resque-bus')
</code></p>

<h4>Application A publishes an event</h4>

<p>Something happens in your application and you want to let the world know. In this case, you publish an event.</p>

<p>```ruby</p>

<h1>business logic</h1>

<p>QueueBus.publish("user_created", id: 42, first_name: "John", last_name: "Smith")</p>

<h1>or do it later</h1>

<p>QueueBus.publish_at(1.hour.from_now, "user_created", id: 42, first_name: "John", last_name: "Smith")
```</p>

<h4>Application B subscribes to events</h4>

<p>If the same or different application is interested when an event happens, it subscribes to it by name.</p>

<p>```ruby</p>

<h1>initializer</h1>

<p>QueueBus.dispatch("app_b") do
  subscribe "user_created" do |attributes|</p>

<pre><code># business logic
NameCount.find_or_create_by_name(attributes["last_name"]).increment!
</code></pre>

<p>  end
end
```</p>

<h3>Upgrading</h3>

<p>The formats changed a little bit with the move. The last version that used the old format (0.3.7) also can adapt to the new format. This is important because you'll have things in the queue during the transition.</p>

<p>Steps:</p>

<ul>
<li>Upgrade everyone to 0.3.7</li>
<li>Deploy all the things</li>
<li>Upgrade everyone to the newest version</li>
</ul>


<h3>More to come</h3>

<p>If people continue to like this approach and gem, we have lots of approaches and tools built on top of it that we'd be excited to make available. Let us know on <a href="https://github.com/queue-bus/queue-bus">Github</a> that you like it by watching, starring, or creating issues with questions, etc.</p>

<p>Now that we have the adapter pattern, also let us know if you are interested in making one for your background system of choice. A special thanks goes out to <a href="https://github.com/jonsgreen">Jonathan Greenberg</a> and the team at <a href="http://www.purpose.com/">purpose.com</a> who did just that to get this whole thing going.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Resque Bus Presentation]]></title>
    <link href="http://bleonard.github.io/blog/2014/02/16/resque-bus-presentation/"/>
    <updated>2014-02-16T08:48:00-08:00</updated>
    <id>http://bleonard.github.io/blog/2014/02/16/resque-bus-presentation</id>
    <content type="html"><![CDATA[<p>{% slide %}</p>

<h2>Resque Bus</h2>

<h3>Brian Leonard</h3>

<h5>TaskRabbit</h5>

<h5>02/17/2014</h5>

<!-- more -->


<p>{% notes %}</p>

<p>Hi, my name is Brian Leonard and I'm the Chief Architect and Technical Cofounder of TaskRabbit. TaskRabbit is a marketplace where neighbors help neighbors get things done.</p>

<p>At TaskRabbit we are using Redis and Resque to do all of our background processing. We've also gone one step further to use these tools to create an asynchronous message bus system that we call Resque Bus.</p>

<p>{% slide %}</p>

<h3>Redis</h3>

<ul>
<li>Key/Value Store</li>
<li>Atomic Operations</li>
</ul>


<br/>


<p><img src="/images/posts/resque-bus-presentation/redis-logo.jpeg" alt="Redis Logo" /></p>

<p>{% notes %}</p>

<p>I don't have to tell you guys about Redis, but for our purposes the main point is that we can store stuff in it and the operations are atomic.</p>

<p>{% slide %}</p>

<h3>Resque</h3>

<p><img src="/images/posts/resque-bus-presentation/resque-screenshot.png" alt="Resque UI" /></p>

<p>{% notes %}</p>

<p><a href="https://github.com/defunkt/resque">Resque</a> is a background queue built on top of Redis. It uses arrays and the atomic properties of Redis to get this done.</p>

<p>The ability to put things in order and then atomically pop an item from the list is all that is really needed for a great background queue and Redis fits perfectly.</p>

<p>There is plenty of code, but it all comes down to inserting json the queue, popping, and executing code with that as an input.</p>

<p>{% slide %}</p>

<h3>Resque Pattern</h3>

<p>```ruby
class Rating &lt; ActiveRecord::Base
  after_commit :enqueue_worker</p>

<p>  def enqueue_worker</p>

<pre><code>Resque.enqueue(CalculateAverageWorker, self.id)
</code></pre>

<p>  end
end
```</p>

<p>```ruby
class CalculateAverageWorker
  @queue = :default</p>

<p>  def self.perform(rating_id)</p>

<pre><code>rating = Rating.find(rating_id)
total  = Rating.where(rabbit_id: rating.rabbit_id).count
sum    = Rating.where(rabbit_id: rating.rabbit_id').sum(:value)

profile = RabbitProfile.find_by(user_id: rating.rabbit_id)
profile.ratings_total  = total
profile.rating_average = sum.to_f / (5*total.to_f)
profile.save!
</code></pre>

<p>  end
end
```</p>

<p>{% notes %}</p>

<p>The pattern goes something like this: something happens on the main (request) thread. It queues up a worker in the background with a class name and arguments. In this case, we enqueue the CalculateAverageWorker with the new rating id. The background worker then does whatever it needs to do. In this case, we update the TaskRabbit's average rating on her profile.</p>

<p>{% slide_top transition: none %}</p>

<p><img src="/images/posts/resque-bus-presentation/rating_resque_1.png" alt="Resque 1" /></p>

<p>{% notes %}</p>

<p>Looking at the Redis data workflow, it looks like this.</p>

<p>{% slide transition: none %}</p>

<p><img src="/images/posts/resque-bus-presentation/rating_resque_2.png" alt="Resque 2" /></p>

<p>{% slide transition: none %}</p>

<p><img src="/images/posts/resque-bus-presentation/rating_resque_3.png" alt="Resque 3" /></p>

<p>{% slide_bottom transition: none %}</p>

<p><img src="/images/posts/resque-bus-presentation/rating_resque_4.png" alt="Resque 4" /></p>

<p>{% slide %}</p>

<h3>New Profile App</h3>

<p><img src="/images/posts/resque-bus-presentation/rating_bus_1.png" alt="Bus 1" /></p>

<p>{% notes %}</p>

<p>As we moved our code to a more service-oriented architecture, we wanted to move profiles to a new Rails app. Most of this app was concerned with uploading media and content and other ways for our TaskRabbits to market themselves. A very minor point is this app is this number to display on a profile. Something like that did not justify <em>not</em> splitting up the app, but it was still something that needed to happen.</p>

<p>{% slide %}</p>

<h3>Resque Bus Pattern</h3>

<p>```ruby
class Rating &lt; ActiveRecord::Base
  after_commit :enqueue_worker</p>

<p>  def enqueue_worker</p>

<pre><code>ResqueBus.publish("rating_created", self.attributes)
</code></pre>

<p>  end
end
```</p>

<p>```ruby</p>

<h1>initializer</h1>

<p>ResqueBus.dispatch("profile") do
  subscribe "rating_created" do |attributes|</p>

<pre><code>rating = Rating.find(rating_id)
total  = Rating.where(rabbit_id: rating.rabbit_id).count
sum    = Rating.where(rabbit_id: rating.rabbit_id').sum(:value)

profile = RabbitProfile.find_by(user_id: rating.rabbit_id)
profile.ratings_total  = total
profile.rating_average = sum.to_f / (5*total.to_f)
profile.save!
</code></pre>

<p>  end
end
```</p>

<p>{% notes %}</p>

<p>So this is more or less the same thing, but with different syntax. The difference is that the rating calculation code can now live in another app. This is possible because of the layer of abstraction that ResqueBus adds.</p>

<p>{% slide_top transition: none %}</p>

<p><img src="/images/posts/resque-bus-presentation/rating_bus_2.png" alt="Bus 2" /></p>

<p>{% notes %}</p>

<p>As you can see, the difference is this new layer of <code>resquebus_incoming</code> that we've added, so now it looks like this:</p>

<ul>
<li>Profile App subscribes to events (puts a hash in Redis saying what it is interested in)</li>
<li>Task App publishes the event (puts published hash as args in a Resque queue called <code>resquebus_incoming</code> with a class of <code>Driver</code>)</li>
<li>The <code>Driver</code> copies the event hash to 0 to N application queues based on subscriptions (arg hash now in <code>profile_default</code> queue with a class of <code>Rider</code>)</li>
<li>The <code>Rider</code> in Profile App executes the block given in the subscription</li>
</ul>


<p>{% slide transition: none %}</p>

<p><img src="/images/posts/resque-bus-presentation/rating_bus_3.png" alt="Bus 3" /></p>

<p>{% slide transition: none %}</p>

<p><img src="/images/posts/resque-bus-presentation/rating_bus_4.png" alt="Bus 4" /></p>

<p>{% slide transition: none %}</p>

<p><img src="/images/posts/resque-bus-presentation/rating_bus_5.png" alt="Bus 5" /></p>

<p>{% slide transition: none %}</p>

<p><img src="/images/posts/resque-bus-presentation/rating_bus_6.png" alt="Bus 6" /></p>

<p>{% slide transition: none %}</p>

<p><img src="/images/posts/resque-bus-presentation/rating_bus_7.png" alt="Bus 7" /></p>

<p>{% slide transition: none %}</p>

<p><img src="/images/posts/resque-bus-presentation/rating_bus_8.png" alt="Bus 8" /></p>

<p>{% slide_bottom transition: none %}</p>

<p><img src="/images/posts/resque-bus-presentation/rating_bus_9.png" alt="Bus 9" /></p>

<p>{% slide %}</p>

<h3>It's Just Resque</h3>

<p><img src="/images/posts/resque-bus-presentation/resque-screenshot.png" alt="Resque UI" /></p>

<p>{% notes %}</p>

<p>The thing to note here is that it's all still Resque workers; which, in turn, is all just moving things around in Redis. This allows us leverage our current infrastructure but with new-found capabilities. The simplicity also allows us use technologies other than Ruby itself. For example, [Evan] just wrote a <a href="http://github.com/taskrabbit/node-resque">resque</a> and <a href="http://github.com/taskrabbit/node-resquebus">resque-bus</a> client in node.</p>

<p>{% slide %}</p>

<h3>Use Cases</h3>

<p>Publishing all all data changes, especially state changes.</p>

<ul>
<li>Rails app communication</li>
<li>Bus Apps</li>
<li>Loggers</li>
</ul>


<p>{% notes %}</p>

<p>We publish all the time, basically every time the data changes. We've found it's particularly important to publish when the state of an object changes. For example a Task goes from the assigned state to the completed state. Some of these events have many subscribers. Many events are completely ignored (at the moment) and that is fine too.</p>

<p>{% slide %}</p>

<h3>Rails app communication</h3>

<ul>
<li>Own codebase and storage mechanisms</li>
<li>+1: Local Simplicity</li>
<li>+1: Local Specialization</li>
<li>-1: Global Complexity</li>
</ul>


<p>{% notes %}</p>

<p>This is like the profile app case. It has it's own codebase and storage to optimize that particular experience. Specifically, it's much more of a document store than the other app, which is more relational.</p>

<p>The separation described here between the two systems involved (Tasks and Profiles) has had a few effects.</p>

<p>Local simplicity has increased. Each system does what it does with simpler code than if it was all combined into the same thing.</p>

<p>Local specialization has increased. For example, now that the profile experience is separate in code, I feel better in choosing the right storage solution for that experience. When in one system, it feels like "yet another thing" added to something that's already complicated.</p>

<p>Global complexity has increased. This separation has a cost and it is in the number of moving pieces. More moving pieces adds maintenance costs through mysterious bugs, time in runtime execution, and overall cognitive load on the developer. It's case by case, but we believe it can be worth it.</p>

<p>{% slide %}</p>

<h3>Bus Apps</h3>

<ul>
<li>No UI, only listens to bus</li>
<li>+1: Local Simplicity</li>
<li>+1: Local Specialization</li>
<li>+1: Memory Profile</li>
</ul>


<p>Examples</p>

<ul>
<li>External Analytics</li>
<li>Communications</li>
<li>Fraud</li>
</ul>


<p>{% notes %}</p>

<p>We have several apps that have no UI and only exist to listen to the bus and take action accordingly. For example, we have a fraud app that might subscribe to the ratings event, along with several others, to help look for fraudulent or "bad" behavior. These apps can be very specialized and focused. It's easy to let a team member own the problem and run with it without worrying about anything else in the system. The straightforwardness allows them to be just a simple ruby process too, which drastically reduces memory usage.</p>

<p>{% slide %}</p>

<h3>Switchboard</h3>

<ul>
<li>Centralized sending email, text messages, push notifications, etc.</li>
<li>No shared knowledge or credentials</li>
<li>Asynchronous API for all apps to use and just focus on the content.</li>
</ul>


<p>{% notes %}</p>

<p>The best example of a bus app is what we call Switchboard. It receives an event that indicates that a text message needs to be sent, so it does so. Or it can look at the user's preference and decide not send it.</p>

<p>Switchboard allows all of the apps in the system to send mails and text messages and push notifications without knowing how to do it. In effect, Resque Bus and Switchboard create an asynchronous API. It's just instead of being a normal API call, it's knowing what to publish to the bus.</p>

<p>In a decentralized system, this adds a lot of value by encapsulating knowledge about Twilio and other external providers and making is dead simple for the various apps to accomplish these tasks. They can just focus on the content and move on.</p>

<p>{% slide %}</p>

<h3>Loggers</h3>

<ul>
<li>Subscribes to everything that's published</li>
<li>Writes to Elastic Search</li>
</ul>


<p>Enabling...</p>

<ul>
<li>Tracing events through system</li>
<li>Simple centralized logging</li>
<li>Realtime business dashboards and metrics</li>
</ul>


<p>{% notes %}</p>

<p>This is a specialized case of the bus app that I think is really interesting. The app subscribes to everything by doing a regex with a <em>dot</em> <em>plus</em>. Any app can now save something to Elastic search by publishing to the bus. We can then also query this index to get all kinds of interesting data like tracing event through the system or adding up revenue for the day on a refreshing screen.</p>

<p>{% slide %}</p>

<h3>Best Practices</h3>

<ul>
<li>Always Be Publishing</li>
<li>Make many small bus apps</li>
<li>Start small</li>
</ul>


<p>{% notes %}</p>

<p>It's fine if an event is completely ignored. Even at the base case, we have recorded what happened in the system for admins to examine later. In our most-used cases, we've seen events like the user signing up to trigger many parallel code paths such as notification, search indexing, fraud checking, analytics analysis, etc. All of this happens while the most important code (the signup itself) has remained unchanged and focused.</p>

<p>The value continues to increase with each subscriber. Small, self-contained, reactive bus apps that process things that can be done asynchronously keep things really tight.</p>

<p>Like any architectural principal, it's easy to overuse something and have it generally get out of control. To get started, start publishing a bunch of things. Then add a few subscribers without your own app. Those "bus apps" can be within your main codebase too. If you're on Rails, maybe within an <a href="http://tech.taskrabbit.com/blog/2014/02/11/rails-4-engines/">engine</a> or even just regular code. See how it works and expand if you like it.</p>

<p>{% slide %}</p>

<h2>Thanks!</h2>

<p>Questions?</p>

<p>{% endslide %}</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Resque Bus]]></title>
    <link href="http://bleonard.github.io/blog/2013/09/28/resque-bus/"/>
    <updated>2013-09-28T16:27:00-07:00</updated>
    <id>http://bleonard.github.io/blog/2013/09/28/resque-bus</id>
    <content type="html"><![CDATA[<p>At <a href="http://www.taskrabbit.com">TaskRabbit</a>, we are using <a href="https://github.com/resque/resque">Resque</a> to do our background job processing. We've also gone one step further and used <a href="http://redis.io/">Redis</a> and Resque to create an asynchronous <a href="http://en.wikipedia.org/wiki/Message_queue">message</a> <a href="http://en.wikipedia.org/wiki/Enterprise_service_bus">bus</a> system that we call <a href="https://github.com/taskrabbit/resque-bus">Resque Bus</a>.</p>

<h3>Redis / Resque</h3>

<p><a href="http://redis.io/">Redis</a> is a single-threaded in-memory key/value store similar to <a href="http://memcached.org/">memcached</a>. Redis has other features like pub/sub and more advanced data structures, but the key feature that makes it an ideal storage engine for a queue and a message bus is that is can perform atomic operations.  Atomic operations are the kind of operations you can expect to do on in-process data (like Array.pop or Array.splice) but in way that keeps the data sane for everyone connected to the database.</p>

<p><a href="https://github.com/resque/resque">Resque</a> is a background queue built on top of Redis. There seems to be <a href="http://sidekiq.org/">other</a> <a href="https://github.com/collectiveidea/delayed_job">options</a> <a href="https://github.com/kr/beanstalkd">out</a> there these days, but we are pretty happy with Resque and associated <a href="http://resquework.org">tools/ecosystem</a>. There is plenty of code in the resque codebase, but it all comes down to inserting json the queue, popping, and executing code with that as an input.</p>

<h3>Resque Bus</h3>

<p><a href="https://github.com/taskrabbit/resque-bus">Resque Bus</a> uses Resque to create an asynchronous <a href="http://en.wikipedia.org/wiki/Message_queue">message</a> <a href="http://en.wikipedia.org/wiki/Enterprise_service_bus">bus</a> system. As we have created more applications with interdependencies, we have found it helpful to create something like this to loosely couple the worlds. There are <a href="http://zeromq.org/">several</a> <a href="http://www.rabbitmq.com/">other</a> <a href="http://redis.io/topics/pubsub">possible</a> <a href="http://kafka.apache.org/">solutions</a> to this problem, but I really felt that it was important to use something that our team understood well for this piece of infrastructure that we could easily modify and maintain.</p>

<!-- more -->


<h4>Application A publishes an event</h4>

<p>Something happens in your application and you want to let the world know. In this case, you publish an event.</p>

<p>```ruby</p>

<h1>business logic</h1>

<p>ResqueBus.publish("user_created", "id" => 42, "first_name" => "John", "last_name" => "Smith")</p>

<h1>or do it later</h1>

<p>ResqueBus.publish_at(1.hour.from_now, "user_created", "id" => 42, "first_name" => "John", "last_name" => "Smith")
```</p>

<h4>Application B subscribes to events</h4>

<p>If the same or different application is interested when an event happens, it subscribes to it by name.</p>

<p>```ruby</p>

<h1>initializer</h1>

<p>ResqueBus.dispatch("app_b") do
  subscribe "user_created" do |attributes|</p>

<pre><code># business logic
NameCount.find_or_create_by_name(attributes["last_name"]).increment!
</code></pre>

<p>  end
end
```</p>

<h3>How it works</h3>

<p>The following is how this workflow is accomplished:</p>

<ul>
<li>Application B subscribes to events (puts a hash in Redis saying what it is interested in)</li>
<li>Application A publishes the event (puts published hash as args in a Resque queue called <code>resquebus_incoming</code> with a class of <code>Driver</code>)</li>
<li>The <code>Driver</code> copies the event hash to 0 to N application queues based on subscriptions (arg hash now in <code>app_b_default</code> queue with a class of <code>Rider</code>)</li>
<li>The <code>Rider</code> in Application B executes the block given in the subscription</li>
</ul>


<p><img src="/images/posts/resque-bus/data_flow.jpg" alt="Redis Bus Data Flow" /></p>

<h4>Dedicated Setup</h4>

<p>Each app needs to tell it's subscriptions to Redis</p>

<pre><code>$ rake resquebus:subscribe
</code></pre>

<p>The incoming queue needs to be processed on a dedicated or all the app servers.</p>

<p>  $ rake resquebus:driver resque:work</p>

<p>The subscription block is run inside a Resque worker which needs to be started for each app.</p>

<pre><code>$ rake resquebus:setup resque:work
</code></pre>

<p>If you want retry to work for subscribing app or you are using hte delayed <code>publish_at</code> syntax, you should run resque-scheduler</p>

<pre><code>$ rake resque:scheduler
</code></pre>

<h4>Combined Setup</h4>

<p>This is the most dedicated way to run it, but all that <code>resquebus:driver</code> and <code>resquebus:setup</code> do is set the <code>QUEUES</code> environment variable. So you could run:</p>

<pre><code>$ rake resque:work QUEUES=*
</code></pre>

<p>That would work only if you have a single app. While I believe this paradigm still adds value for a single app, it's likely you have more than one app and the most important rule is to not allow Application C to process Application B's queue, so that command would likely look more like this:</p>

<pre><code>$ rake resque:work QUEUES=app_b_default,resquebus_incoming
</code></pre>

<p>It's best practice to set your queue names, anyway. If you use resque-bus in the same Redis db as your "normal" Resque queues, then your full command set would probably look something like this:</p>

<pre><code>$ rake resquebus:subscribe
$ rake resque:work QUEUES=high,app_b_default,medium,resquebus_incoming,low
$ rake resque:scheduler
</code></pre>

<h4>It's Just Resque</h4>

<p>The above illustrates the primary reason that I like this system. <strong>It's just Resque</strong>.  While this may not be the most performant way to create a message bus, there are a number of good reasons to do so:</p>

<ul>
<li>Nothing new to monitor or deploy</li>
<li>If used in a combined setup, you have nothing new to run</li>
<li>If it stops processing a queue (downtime, during deploy process), it catches back up easily</li>
<li>I understand what is going on (and resque has a simple data model in general)</li>
<li>It's portable.  Resque has been re-implemented in a number of languages beyond ruby (we use a node.js rider for example)</li>
<li>Many plugins already exist to add in extra capabilities (stats recording for example)</li>
</ul>


<p>I feel that the "I understand point..." sounds a little like <a href="http://en.wikipedia.org/wiki/Not_invented_here">NIH</a>, but it's just really important to me to fully know where this critical data lives.</p>

<p>Of course, because it's just Resque, there are known issues to work through:</p>

<ul>
<li>It's relatively slow when compared with other systems. We've experimented with Node and Sidekiq to do the <code>Driver</code> role if this becomes an issue.</li>
<li>Redis does not have a good failover system so this adds a single point of failure to the system. We've been working on various techniques to mitigate this risk including replication and (failover tools)[https://github.com/twitter/twemproxy].</li>
</ul>


<h3>Use Cases</h3>

<p>The effect on our apps from other apps publishing and subscribing ends up being one of focus. A request comes in to the web server and that code is in charge of accomplishing the primary mission, for example signing up a user. When this is finished, it publishes an event called <code>user_created</code> just in case other apps care.</p>

<p>Sometimes one or several apps do care. In the signup case, our marketing app subscribes and starts a campaign to onboard that user as effectively as it knows how starting with a welcome email. Our analytics app subscribes and lets various external systems like Mixpanel know. Our admin search tool subscribes to get that user in the index. And so on.</p>

<p>Most of our data goes through certain states. For example, a Task goes from assigned to completed. Overall, we have found that publishing when the states changes is just about always the right thing to do. Some of those events have many subscribers. Many events are completely ignored (at the moment) and that is fine too.</p>

<p>A few types of apps have evolved within this paradigm:</p>

<ul>
<li>Rails apps that subscribe and publish in order to achieve their goals</li>
<li>Bus apps that are small and data driven that have no UI</li>
<li>Logging and analytics apps that subscribe to record many events</li>
</ul>


<h4>Rails app communication</h4>

<p>When a Task is posted on the site, the app publishes a <code>task_opened</code> event. This is a very important event and there are lots of subscribers. One of them is our Task-browsing app that helps TaskRabbits find the right job for them. It has its own codebase and storage designed to optimize this particular experience. When it receives the event about the new Task, it does all the calculations about who is right for the job and stores them in the way it wants to optimize the browsing. It is also subscribed to events that would indicate that the Task is now longer to be browsed by TaskRabbits. In these cases, it removes objects related to that Task from storage.</p>

<p>The separation described here between the two systems involved (Task posting and browsing) has had a few effects.</p>

<ul>
<li>Local simplicity has increased. Each system does what it does with simpler code than if it was all combined into the same thing.</li>
<li>Local specialization has increased. For example, now that the browsing experience is separate in code, I feel better in choosing the right storage solution for that experience. When in one system, it feels like "yet another thing" added to something that's already complicated.</li>
<li>Global complexity has increased. This separation has a cost and it is in the number of moving pieces. More moving pieces adds maintenance costs through mysterious bugs, time in runtime execution, and overall cognitive load on the developer. It's case by case, but we believe it can be worth it.</li>
</ul>


<p>Finally, note that this Rails app also publishes events about the new TaskRabbits that are relevant to the Task.</p>

<h4>Bus Apps</h4>

<p>Specifically, the browsing application publishes N events, each about a notification that should occur because of the new Task. We have a class of application which has no UI and just listens on the bus. We call the app listening for notification events Switchboard. Switchboard is an example of what I called a "bus app." A bus app exists to subscribe to various events and take action based on the data. In this case, Switchboard receives an event that indicates that a text message needs to be sent, so it does so. Or it can look at the user's preference and decide not send it.</p>

<p>With this approach, Switchboard is able to accomplish a few things effectively:</p>

<ul>
<li>It is the only app that knows our <a href="https://www.twilio.com">Twilio</a> credentials or how to format the HTTP call</li>
<li>It is the only one that knows that we even use Twilio or what phone number(s) to send from</li>
<li>It is the only app that decides what phone number to send to and/or how to look up a user's preferences</li>
<li>It can have a drastically reduced memory profile than a normal Rails app in order to be able to process more effectively.</li>
<li>It provides a centralized choke point for all outgoing communications, making something like a staging whitelist easy to implement</li>
</ul>


<p>In effect, ResqueBus and Switchboard create an asynchronous API. Simply knowing the terms of the API (what to publish) provides several benefits to the consuming apps:</p>

<ul>
<li>They don't have to know how to send text messages</li>
<li>They don't have to know how to look up a user's preferences or even phone number</li>
<li>They don't have to change anything if we decide to send text messages differently</li>
<li>They can focus on the content of the message only</li>
<li>They will not be help up or crash if Twilio is having a problem of some sort</li>
</ul>


<h4>Loggers</h4>

<p>As noted, all of these benefits of decentralization come at the cost of global complexity. It's important to choose such architectural areas carefully and clearly this approach is one that we've fully embraced. The addition of these "additional" moving pieces requires creation of new tools to mitigate the operational and cognitive overhead that they add. A good example that I read about recently was the ability Twitter has to <a href="http://highscalability.com/blog/2013/7/8/the-architecture-twitter-uses-to-deal-with-150m-active-users.html">trace a tweet</a> through the whole lifecycle.</p>

<p>At TaskRabbit, the equivalent is an app called Metrics that subscribes to every single event. Case by case, the Metrics subscription adds some data to assist in querying later and stores each event.  We store events in log files, and optionally, <a href="http://www.elasticsearch.org/">elastic search</a>. When combined with <a href="https://github.com/taskrabbit/resque-bus/blob/9171f0879e1c356e64086230d71698a52304f8d7/lib/resque-bus.rb#L153">unique ids</a> for each event that subscriptions can chain along if they republish, this provides the capability to trace any logical event through the system.</p>

<p>That was the original goal of the system, but it somewhat accidentally had several effects.</p>

<ul>
<li>Again, the ability to trace a logical event throughout decoupled systems</li>
<li>Centralized logging capability a la <a href="http://www.sumologic.com/">SumoLogic</a> for free (any app can publish random stuff to bus)</li>
<li>With minor denormalization and well-crafted queries, realtime business dashboards and metrics a la <a href="https://mixpanel.com/">Mixpanel</a> or <a href="https://www.google.com/analytics">Google Analytics</a></li>
</ul>


<h3>Subscriptions</h3>

<p>There are a few other ways to subscribe to receive events.</p>

<h4>Any Attributes</h4>

<p>The first version of Resque Bus only allowed subscribing via the event type as show above. While I found this covered the majority of use cases and was the easiest to understand, we found ourselves subscribing to events and then throwing it away if other attributes didn't line up quite right. For example:</p>

<p>```ruby
subscribe "task_changed" do |attributes|
  if attributes["state"] == 'opened'</p>

<pre><code>TaskIndex.write(attributes["id"])
</code></pre>

<p>  end
end
```</p>

<p>While this is fine, something didn't sit quite right. It adds unnecessary load to the system that could have been avoided at the <code>Driver</code> level. The biggest realization is that <code>bus_event_type</code> is no different than any other attribute in the hash and doesn't deserver to be treated as such.</p>

<p>In the current version of Resque Bus, this code is now:</p>

<p>subscribe "any_id_i_want", "bus_event_type" => "task_changed", "state" => "opened" do |attributes|
  TaskIndex.write(attributes["id"])
end</p>

<p>This ensures it never even makes it to this queue unless all of the attributes match. I felt it was important to keep the simple case simple (so it's still possible), but in the implementation the first subscription is equivalent to this:</p>

<p>```ruby
subscribe "task_changed", "bus_event_type" => "task_changed" do |attributes|
  if attributes["state"] == 'opened'</p>

<pre><code>TaskIndex.write(attributes["id"])
</code></pre>

<p>  end
end
```</p>

<h4>Subscriber Mixin</h4>

<p>It feels really powerful and magical to put code like this in a <a href="http://en.wikipedia.org/wiki/Domain-specific_language">DSL</a> in your initializer or other setup code. However, when we started creating apps that had many subscriptions, it got to be a little overwhelming. For this we created an Object mixin for subscription.</p>

<p>```ruby
class TaskChangesSubscriber
  include ResqueBus::Subscriber
  subscribe :task_changed
  subscribe :changed_when_opened, "bus_event_type" => "task_changed", "state" => "opened"</p>

<p>  def task_changed(attributes)</p>

<pre><code># gets called for all task changes
</code></pre>

<p>  end</p>

<p>  def changed_when_opened</p>

<pre><code># only gets called when state == "opened"
</code></pre>

<p>  end
end</p>

<p>```</p>

<p>This really cleaned up subscription-heavy apps.</p>

<p>Note: This subscribes when this class is loaded, so it needs to be in your load or otherwise referenced/required during app initialization to work properly.</p>

<h3>More to come</h3>

<p>If people seem to like this approach and gem, we have lots of approaches and tools built on top of it that I'd be excited to make available. Let us know on <a href="https://github.com/taskrabbit/resque-bus">Github</a> that you like it by watching, starring, or creating issues with questions, etc.</p>
]]></content>
  </entry>
  
</feed>
